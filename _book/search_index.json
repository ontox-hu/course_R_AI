[["index.html", "Intro into R, Reproducible research and machine learning Introduction Course outline Learning objectives", " Intro into R, Reproducible research and machine learning Introduction Welcome to the course on the basics of data analysis in R and an intro in Machine learning. This course consists of 5 lessons. You can find these lessons in the menu on the left. Each lesson starts with a short introduction by the teachers. After that, you are supposed to study the lessons yourself and make the exercises. During the lessons, you can ask questions to the teachers and the teachers will provide feedback. It is very important that you make all exercises. You will not learn using R by reading about it. Please note: this is a crash course. We scheduled this course to allow you to throw yourself into R and be able to see yourself making progress very quickly. We wish you good luck and we hope you will have a lot of fun with R! Course outline This page explains how the course is constructed and what will be expected from you. Schedule 2022: Day 1: Intro to R and RStudio Getting the materials from Github R-syntax, objects, data types and classes RMarkdown Day 2: Reading and cleaning data Visualizations Day 3: Data wrangling Day 4: Functional programming Writing functions Day 5: Machine Learning with {tidymodels} Deep Learning with tensorflow for R Course material The course material consists of the following: Instruction pages (menu to the left). Exercises. To navigate through the different materials, you can use the menu on the left. The course is based on the online book R for Data Science. Lessons Each lesson will consist of: a short introduction to the subject by the instructor; selfstudy of the instruction pages; and making the exercises. Each lesson, one or two instructors will be present to assist with the exercises and provide feedback. Exercises The exercises consist of writing R code to solve data analysis problems. Answers are available for each exercise. Most likely you will not finish all exercises during the lesson, which means that you will have to finish them in your own time. It is important to make all exercises: the only way to learn R is to practice, practice and practice by making the exercises. Some R functions and principles are only explained via the exercises. Rstudio server During the course, we will use an RStudio server, which you can reach via this link using a browser. You can log in to this RStudio server with your HU email address and with your HU password. Within the Rstudio environment you’ll have to make an R Project (see instructions in lesson 1). Data files will be provided by the instructors and are present in the Rstudio server folder mabdaur/data. In case of any problems with the RStudio server, please contact marc.teunis@hu.nl. Learning objectives After this course, you will be able to use R to perform your analyses repoducible backup your work using version control in github implement literate programming in data analysis and visualization. communicate the results of data analysis and how you reached them using Rmarkdown explain the basics of machine learning in toxicology. "],["preparation-before-the-course.html", "Preparation before the course Let’s get started! Updating R and RStudio Install R Install RStudio If all is still not well reader Introduction to R Rstudio Projects DIY", " Preparation before the course Let’s get started! Note: if you already have R and RStudio installed (and recently updated) on your computer, skip to the next page RStudio is a nice IDE for using R, with a user-friendly interface and lots of options. In preparation of DAUR2, we are going to install RStudio (and first R itself!) on your own computer, so it is readily available to you at any time within and after this course. R and RStudio are both free and open source. We encourage you to keep using R for future projects, internships and your everyday data issues. It is currently an important tool in data science and bioinformatics (and looks rather nice on your resume). Getting it all set up can be a bit tricky at times, so please follow the instructions carefully and in the correct order (seriously, keep to the order as well, it will make your life easier). The instructions will walk you through it. Remarks before starting Windows users Please make sure you are allowed to install software on your computer. If you do not have administrator rights for the computer you are using, you can contact the teachers for guidance. Mac users Mac security settings might block you from installing certain programs. Whenever you see a pop-up like this: Go to you Applications –&gt; System Preferences –&gt; Security&amp;Privacy Click on “Open anyway” and in the next pop-up on “open”. Updating R and RStudio Note: if you you haven’t installed R and RStudio yet, you do not need to do this. Skip to installing R. Windows users Update RStudio Go to Help &gt; Check for Updates. Update R Open RStudio, and type in the console: install.packages(&quot;installr&quot;) library(installr) Let it install and load, and type: updateR() This will check whether you have the current version, and update if not. It will guide you through the decisions; say “yes”, “OK” and/or “next” whenever it wants your opinion. After this, skip the next two sections and continue to the page on installing git. Mac users Update RStudio Actually, just open RStudio. It will warn you if there are updates, unless you disabled this option in the preferences menu; in that case you can go to Help &gt; Check for updates. Update R Type R.version.string in the console, check here if that is correct. If it needs to be updated, install the newest version of R as described on the next page. When you are done, close and reopen RStudio. Install R Windows Go to the R website and click on the download link on top. You will download a file called “R-3.6.3-win.exe” or very similar. Run this file and follow all steps using the default settings to install R on your computer. Mac Go to the R website. If you have the latest OS X, ignore all text until “Latest release:”, and click on the top .pkg file to download it. If you do not have the latest OS X on your mac, consider updating it first. If you really do not want to, find the .pkg file for your OS X in the table and download it. Open the .pkg file to start the installer and follow all steps using default settings to install R on your computer. Some functions in R that you may use in the future require something called X11, which used to be included in OS X, but isn’t anymore. To prevent future trouble, let’s install it via XQuartz while we are installing things anyway: Go to https://www.xquartz.org/, download the file under “quick download”. Open the downloaded disk image (.dmg file). Double click on XQuartz.pkg and follow the installation steps. Install RStudio Install RStudio itself Go to the RStudio website The website will probably suggest the correct download file for you (A). If not, look in the table lower down the page for the correct file (B): Download and run the installer with default installation options: Now open your newly installed RStudio. It will look familiar: Installing packages However, not everything is up and running as it was on the server in DAUR1. First of all, not all packages that we use have been installed. You will have to install packages once on your computer, and load them within every script you want to use them. To install packages, use the console. Do not install packages from within a script (the figure below shows RStudio when a script is open to highlight the difference): Go ahead and install the tidyverse package, we will need it during the course. Type the following command in the console: install.packages(&quot;tidyverse&quot;) If, in the future, R bugs you in the console with “Do you want to install from sources the packages which need compilation?” just answer no (n) for now. Loading packages A quick note on loading packages, even though you don’t have to do that now. Remember that packages (in this case the tidyverse package) can be loaded using: library(tidyverse) Make it a habit to load all packages you need in the beginning of your script, instead of loading them where-ever you happen to need them. This makes it easier to keep track of all the packages you are loading and prevents you from loading and reloading packages every time you run a chunk of code along the way. If all is still not well Are you experiencing technical problems, software that does not run or options that are not available, or any other type of problem, check below: In the following order: Have you tried turning your computer off and on again? Turn off your computer and start it back up. Try if it works now. Try to find your problem on the internet. If RStudio gives you an error message, copy paste (part of) the error message to google and see if a solution comes up. Really: try, click around, see what you can come up with, using the help of the internet. Consider this part of what you gain from this course, being able to google and solve R trouble is a valuable skill. And you will get better in it the more you practice (we know, we’ve practiced a lot). If 1, 2 and 3 all fail, send us an e-mail describing your problem, including a copy-paste of the error message if there is one (no screenshot). reader Obviously, you have already found the reader for the course. Try the following: Open links in the reader in a new tab by holding ctrl (pc) or command (mac) when clicking on it. You can click menu-items to see the different paragraphs. Introduction to R R is a language and environment for statistical computing and graphics. It is one of the most popular programming languages for data analysis and statistics, rapidly taking over software packages such as SPSS. Rather than using a GUI (Graphical User Interface, clicking buttons and menus), R is typically accessed through a command-line interpreter. So you will type for instance “2+2” and R will tell you the answer: 2+2 ## [1] 4 However, you can also ask R to give you p-values for an ANOVA, print you a nice scatter plot, or give a summary of a large dataset. Once you know how, R will let you do these tasks a lot faster than Excel or SPSS could. Moreover, more advanced users will be able to ask R to do things like finding all the genes in an online database that are associated with a certain disease, fit deep learning models, build interactive web apps and analyse huge next generation sequencing datasets. R is thus extremely versatile, but can be a bit daunting when first encountered. But as you likely have some experience in SPSS and Excel, you already know the basics of data analysis. You will see that learning R will save you a lot of time in the future. Also, R has been adopted by many companies as the language of choice to analyze data, so it is an important skill to acquire. Therefore, we will give you an introduction to the R language in this course. R is an open-source project supported by the user community, so please note that it is all free of charge. This also means that R changes fast depending on the needs of the user community. We may encounter instances where things have changed in the time window between the development of this course and you following the lessons. Consider these differences a valuable lesson in really getting to know R and its possibilities. Variables and functions The standard R installation comes with base R and core packages that allow for a fully functioning statistical environment. Rather than allowing the user to look at data in a two-dimensional data table on screen, such as for instance in Excel or SPSS, R saves data in variables. Variables can contain arrays of numbers, or even lists of multiple data tables. These variables are stored, but not printed on the screen unless you ask R to do so. We will further discuss them in lesson 1. Once you have stored your data in variables, you can apply functions to your data. In programming, functions are sets of instructions that you want to use repeatedly. Many functions are stored and shared in packages. A package provides code, functions and data to execute certain tasks in R. There are hundreds of functions each performing a certain task. For example the function sum() from the base package calculates the sum of a series of numbers. You can find a complete list of base R function and their descriptions on this website. Beside the base R functions, R comes with some core packages. These core packages include the stat package, which can be used to perform statistical tasks. For example, the function median() calculates the median of a series of numbers. Figure 1: Organization of the R workflow Rstudio Open up your newly installed Rstudio program Rstudio provides a user-friendly interface with separate windows for everything you will be using: the console, scripts, environments, objects, graphs, files and so on. Your screen will look like this: Figure 2: Rstudio layout Console In the bottom left you see the console window and terminal. You will be working mostly in the console window, in which the line should start with &gt; (if it starts with root@something_something_your.name# you are typing in the terminal. Go to the console by clicking on “console” in the bottom left menu bar). Note (seriously!) that anything you type in the console will not be saved. This is just where you can tell R what to do right now, and where R will print the results of a command. You can use it as a calculator as well. try typing 2+2 in the console window and hit enter These commands typed directly into the console will be forgotten when you close the session. A session is in fact just this: R keeps track of your calculations and the variables you made (we’ll get to that) until you explicitly quit the session. Within the current session Rstudio will also remember your console commands. You can retrieve previous commands with the arrow keys: Up — Recall previous command(s) Down — Reverse of Up Try pressing the UP key. You will see that R retrieves the 2+2 command. You can use this to repeat commands (or change them a bit if you want). Projects Most of the time, we do want R to save what we are doing. Therefore, we will be working in scripts, and within a project. If you start with R you should create a Project. A project defines a folder where you store all the datafiles, Rscripts and RMarkdown files. When you open a Project your working directory will be the folder where your Project file is stored. Projects can be exchanged between computers and researchers in time as it contains all the necessary packages, files and scripts to do the analysis. A project is self-contained. A researcher can define multiple Projects within Rstudio which run independently of each other. A Project also guarantees that datafiles /scripts can be found in subfolders within the Project (even when executed on a different computer) Starting a new Project in Rstudio Click on File and Select New Project If you encounter this message Click on “Don’t Save”, otherwise continue with the next instruction Click on “new Directory” Make a new directory for this course (for instance “daur” for Data Analysis Using R) Click on “Create Project” Done Opening a project in Rstudio When starting an R session open the Project you want to work on. This will automatically set your working directory to the folder containing the RProject file. The working directory is where R will look for any files (datafiles, scripts etc) you want to use. If you want to switch projects, or you are not currently in a project, you can open one: Click on “Open Project”. If you have opened the RProject before you can also select “Recent Projects” Select and open the folder containing the RProject file. In this instance daur Select and open the daur.Rproj file to open the RProject We will continue working in Rstudio in lesson 1. More help This course is based on the book “R for Data Science” of Garrett Grolemund and Hadley Wickham. This book is freely available. We will refer to this book in the learning material providing extra (in depth) information / explanation. There are thousands of tutorials on R if you want to know more. Generally, online tutorials on R: are rather good. Good starting points are: bioconductor Rstudio Coursera Rmarkdown The official documentation on R functions can be difficult to read / understand. To start searching for information on R, for example on how a particular function works you can use google (or your favourite searching engine) with the following search term: R function() tutorial / example So for example, google: R unique() tutorial / example DIY In this course, we will give you a tour of R and we will discuss many functions using examples. Every lesson will consists of a series of canvas pages with examples and explanations, followed by exercises. We expect you to try things yourself during this course. When reading the lessons, try all the examples in R. So open Rstudio on the server, copy-paste the examples in a script and run them. Play around with the code. You will not learn R: by watching someone else write it, or by reading about it. This is a skill that you will need to practice. Often, we will show you what the output of a piece of code should look like in lighter gray. You can also recognise code output by the ## in front of the lines: Example sesamstraat &lt;- c(&quot;ernie&quot;, &quot;bert&quot;, &quot;elmo&quot;, &quot;pino&quot;, &quot;grover&quot;, &quot;kermit&quot;, &quot;ms piggy&quot;, &quot;koekiemonster&quot;) sort(sesamstraat) ## [1] &quot;bert&quot; &quot;elmo&quot; &quot;ernie&quot; &quot;grover&quot; &quot;kermit&quot; &quot;koekiemonster&quot; ## [7] &quot;ms piggy&quot; &quot;pino&quot; If your output looks different, such as: Error in sort(sesamstrat) : object 'sesamstrat' not found you know something is going wrong. Make sure you code along with the examples. Try to replicate what the reader is doing! "],["lesson-1---introduction-to-r-and-rstudio.html", "Lesson 1 - Introduction to R and RStudio Learning objectives Starting to work within a project Data types and structures in R Indexing Missing values More complicated data Lists Dataframes Tibbles Factors using functions: drc scripts and Rmarkdown Rmarkdown bla YAML header Code chunks Output control Plain text formatting Here package Creating a new RMarkdown file", " Lesson 1 - Introduction to R and RStudio Learning objectives After this lesson: you know how RStudio is organized; you can run R scripts in RStudio using R scripts; you know how to use R packages; and you know how you can find information (help function and vignettes) about the R functions you are using. You will know the different data types and structures in R. You can create your own R objects. Make sure you code along with the examples. Try to replicate what the reader is doing! Starting to work within a project In the preparation for this course, you made a project. Open this project. Running code in Rstudio To execute R functions / code there are two options: Directly in the R console (the code is not saved) Write a script. (the code can be saved) The preferred method is to enter the code in a script. Writing and executing R code by using a script has several advantages: 1 The code can be saved and therefore is reproducible. 2 The code can be shared between researchers 3 The code can be made clear by adding comments using the # sign 4 The code can be re-used for other purposes In contrast to R script, your code will be lost if you directly enter and execute the code in the R console. To create an R script: click on the “green” plus sign and select R script (see Figure 3) Figure 3: creating a new R script in Rstudio A new script has opened in the upper left part of the screen. Try typing 2+2 in your new script To execute this command: Put the cursor on the line of code of interest and press Ctrl + enter alternatively: Make a selection of lines of code and press Ctrl + enter. To source the entire script press ctrl + shift + s To run the entire script, click the green arrow with “run” next to it on the top right corner of your script. The OUTPUT of the line of code will be visible in the R Console or if making a graph in the Plots panel in the lower right window (see Figure 4)** Figure 4: Example of an R script **Read the code + comments in figure 4 carefully to understand the principle of an R script. Two object were created and saved in the Environment panel in the top right. Importantly, within an R script any text starting with a # sign will be treated as text only. This is very useful to write comments on why you’re using a particular code. The actual lines of code are: 2*4 3^0.5 ## ^ sign means to the power to division1 &lt;- 100 / 4 ## create an object named division1 division2 &lt;- division1 / 5 ## create an object named division2 based on object division1 (division1 - division2) / division1 ## you can use brackets To save an R script simply press Ctrl+S or click on file and click on the save or save as options. The code will be saved in your RProject folder Variables Copy and paste the lines in the grey block above into a new script and run the script (select the lines and press ctrl+enter OR click on “Run” on the top right above your script). We have made two variables in the 5 lines of code (or objects, anything in R is an object. On the internet, you will see “variable” and “object” used interchangeably when talking about R:. What are known as objects in R are known as variables in many other programming languages). You can find all objects created in a session in the upper right window, within the “environment” tab. (find the environment tab) If you ran the script correctly, division1 and division2 should be visible there (see also figure 4 above). Objects are created with the assignment operator &lt;- It assigns the stuff on the right to an object on the left. So, after running division1 &lt;- 100 / 4 , the value of division1 is 25. We will dive into this in lesson 2. Type division1 into the console window to see its contents. Exercise 1 Write an R script (so not in the console) that calculates the 95% confidence interval from a sample with 60 datapoints (n) with a given sample mean and sample standarddeviation. Follow the steps below. The equation for a 95% confidence interval is: \\(xgem \\pm 1,96 * \\frac{s}{\\sqrt{n}}\\) with: \\(xgem\\) : sample mean, \\(s\\): sample standard deviation, \\(n\\) : amount of measurements Do the following: open a new R script first make 3 objects, on 3 separate lines: xgem contains the number 50 s contains the number 4 n contains the number 60 on the fourth line, make an object that contains the square root of n (remember that \\({\\sqrt{n} = n^{0.5}}\\)) on the fifth and sixth line, write the equations for the lower and upper boundary of the 95% confidence interval. Use the objects you made. Don’t forget the brackets () ! Click for the answer xgem &lt;- 50 s &lt;- 4 n &lt;- 60 sqrootn &lt;- n^0.5 xgem - (1.96*(s/sqrootn)) # lower ## [1] 48.98786 xgem + (1.96*(s/sqrootn)) # upper ## [1] 51.01214 Functions Exercise 1 Make a new variable called length_myname, which contains a number representing the length of your first name. For example, type in the script: length_myname &lt;- 7 Click for the answer # for example if your name is &quot;Bas&quot; : length_myname &lt;- 3 We can actually ask R: to calculate the length if we want. Type in the script (replace my name with yours) and run the lines (select them and press Ctrl + enter): myname &lt;- &quot;alyanne&quot; nchar(myname) The number of characters in your name should appear in the console. We have just used a function. Functions allow you to automate common tasks, such as count the characters in a word, or calculate the mean of a series of numbers. Functions generally want some input (such as the word or the numbers), which is placed between round brackets: function_name(input) For instance, there is a function to calculate the square root of something, called sqrt(): sqrt(25) ## [1] 5 Packages and libraries R has many build in functions, but you add more by writing them yourself (we won’t cover that in this course) or by installing packages with functions other people have written. The R: community is very active, and also very open-source. There are a lot (!) of packages available online for free. In this course we have already preinstalled most packages you need. A package needs to be loaded in R though, by the function library(). A package only needs to be installed once but needs to be loaded every time you have closed Rstudio on the server. Start all our scripts with loading the packages you are going to use. A package that we will extensively use during this course is named “tidyverse”. So start your scripts within this course with: library(tidyverse) The help function Each function in R operates with a set of arguments. To check how a function in R operates, type a question mark in the console followed by the name of the function. For example to generate a sequence of numbers we use the function seq(). To see how it works type: ?seq() Data types and structures in R Now that you have met Rstudio, we can begin working with data! Data structures in R Data needs to be stored and therefore must be organized in (complex) data structures. The most important data structures in R are: vector; collection of values of the same data type. factors; a vector with categorical data, can contain only predefined values. list; elements in lists can be of any type, including lists. dataframes &amp; tibbles : sort of like a table in excel. Tt has rows and columns (variables). objects are created with the assignment operator: &lt;-. name_of_object &lt;- what_you_want_to_store_in_it Object names must start with a letter and can only contain letters, numbers, ’_’ (an underscore) and ‘.’ (a dot). Object names must be informative so that you remember what kind of data is stored in the R object. A convention is to use snake_case where you separate lowercase words with underscores. For example, a possible object name for a gene expression dataset in stem cells could be ‘rnaseq_mouse_24hrs_stemcells’. Exercise 1 Now that you know how to assign things to an object ( &lt;- ) and use a function (functionname(input)), let’s try the following: Write R code to calculate: the sum of even numbers between 30 and 58. the square (power of 2) of the numbers 1 till 50. Note: the power function in R is written as a^b meaning a to the power of b. For example 2^3 = 2x2x2 = 8. the square (power of 2) of the odd number between 50 and 100. Click for the answer sum(seq(30,58,2)) ## [1] 660 (1:50)^2 ## [1] 1 4 9 16 25 36 49 64 81 100 121 144 169 196 225 256 289 324 361 400 ## [21] 441 484 529 576 625 676 729 784 841 900 961 1024 1089 1156 1225 1296 1369 1444 1521 1600 ## [41] 1681 1764 1849 1936 2025 2116 2209 2304 2401 2500 odd_numbers &lt;-seq(51, 100, by=2) odd_numbers^2 ## [1] 2601 2809 3025 3249 3481 3721 3969 4225 4489 4761 5041 5329 5625 5929 6241 6561 6889 7225 7569 7921 ## [21] 8281 8649 9025 9409 9801 Indexing Quite often, you will want to use a subset of a vector for further analysis. You can select elements from a vector by using an index inside a single square bracket “[]” operator. This index can be numbers (the positions of the elements to be selected), the first element of an vector has index 1 and so on. But it can also be for instance a function or logicals. Example long_vector &lt;- c(1:15,5,6,5,6) long_vector ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 5 6 5 6 # Positional indexing: Display the second to fifth element of lange_vector long_vector[2:5] ## [1] 2 3 4 5 # Take the sum of the second to fifth element of lange_vector sum(long_vector[2:5]) ## [1] 14 # Positional indexing: Display element 2, 6 and 8 of the built-in object letters letters[c(2,6,8)] ## [1] &quot;b&quot; &quot;f&quot; &quot;h&quot; # function: Display the last element in lange_vector lange_vector[length(lange_vector)] ## [1] 6 # Logical: Display the elements in lange_vector that are equal to 5 lange_vector[lange_vector==5] ## [1] 5 5 5 Most R functions are vectorized. That means that the functions can take vectors as input: the function operations are applied to all vector elements. Example vector_1 &lt;- c(1, 2, 3, 4, 5) vector_1 + 10 ## [1] 11 12 13 14 15 vector_1 * vector_1 ## [1] 1 4 9 16 25 sum(vector_1) ## [1] 15 mean(vector_1) ## [1] 3 To select elements of a vector based on a condition we can use indices [] and the function subset(). ###### Example # Create a vector with the seq() function # NB: length.out determines the number of elements reeks &lt;- seq(0, 10, length.out = 6) # Select the elements larger than 5 reeks[reeks &gt; 5] ## [1] 6 8 10 reeks2 &lt;- seq(100, -100, -10) # Extract the vector elements smaller or equal to 30 subset(reeks2, reeks2 &lt;= 30) ## [1] 30 20 10 0 -10 -20 -30 -40 -50 -60 -70 -80 -90 -100 # Extract the vector elements between -30 and 30 # NB: &#39;&amp;&#39; means &#39;and&#39; subset(reeks2, reeks2 &gt;= -30 &amp; reeks2 &lt;= 30) ## [1] 30 20 10 0 -10 -20 -30 # Extract the vector elements smaller or equal to -50 or # larger or equal to 50 # NB: &#39;|&#39; means &#39;or&#39; subset(reeks2, reeks2 &lt;= -50 | reeks2 &gt;= 50) ## [1] 100 90 80 70 60 50 -50 -60 -70 -80 -90 -100 To replace elements of a vector we use the square brackets []. Example # Create a new vector called sesamstraat_1 sesamstraat_1 &lt;- c(&quot;pino&quot;, &quot;elmo&quot;, &quot;bert&quot;, &quot;ernie&quot;) sesamstraat_1 ## [1] &quot;pino&quot; &quot;elmo&quot; &quot;bert&quot; &quot;ernie&quot; # Replace the first element &quot;pino&quot; with &quot;gonzo&quot; sesamstraat_1[1] &lt;- &quot;gonzo&quot; sesamstraat_1 ## [1] &quot;gonzo&quot; &quot;elmo&quot; &quot;bert&quot; &quot;ernie&quot; # Replace element 3 (&quot;bert&quot;) and 4 (&quot;ernie&quot;) with &quot;groover&quot; and &quot;kermit&quot; sesamstraat_1[3:4] &lt;- c(&quot;groover&quot;, &quot;kermit&quot;) sesamstraat_1 ## [1] &quot;gonzo&quot; &quot;elmo&quot; &quot;groover&quot; &quot;kermit&quot; To add elements to a vector, we use the function append(). # Add the elements &quot;ernie&quot; en &quot;ms piggy&quot; to the end of the vector append(sesamstraat_1, c(&quot;ernie&quot;, &quot;ms piggy&quot;)) ## [1] &quot;gonzo&quot; &quot;elmo&quot; &quot;groover&quot; &quot;kermit&quot; &quot;ernie&quot; &quot;ms piggy&quot; # Add the elements ernie&quot; en &quot;ms piggy&quot; after the 3rd element of the vector append(sesamstraat_1, c(&quot;ernie&quot;, &quot;ms piggy&quot;), after = 3) ## [1] &quot;gonzo&quot; &quot;elmo&quot; &quot;groover&quot; &quot;ernie&quot; &quot;ms piggy&quot; &quot;kermit&quot; Exercise 1 Write R code to select the vowels from the R contant LETTERS. Use subset() and the bolean operator ‘|’ . Click for the answer subset(LETTERS, LETTERS ==&quot;A&quot; | LETTERS ==&quot;E&quot; | LETTERS ==&quot;I&quot; | LETTERS ==&quot;O&quot; | LETTERS ==&quot;U&quot;) ## [1] &quot;A&quot; &quot;E&quot; &quot;I&quot; &quot;O&quot; &quot;U&quot; Missing values Most often you will import data in R containing missing values. Missing values are important to recognize because they interfere with arithmetic calculations. Missing values are generally denoted as NA in R objects. Example # Create a vector containing 1 missing value (NA) vector_with_na &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, NA) # The sum and median of the vector will be NA # (because there is an NA in the vector) sum(vector_with_na) ## [1] NA median(vector_with_na) ## [1] NA # The summary of the vector will have NA as part of the output summary(vector_with_na) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1.00 3.25 5.50 5.50 7.75 10.00 1 There are several ways of dealing with missing data and which one is best will depend on why the data is missing. This is beyond the scope of the current course. The most common method is to just ignore the missing values. This can be a reasonable strategy. There are several solutions to ignore missing values in R objects: Create a vector without the missing values. Ignoring the NA values during the calculation. Example # Create a new vector without NA values # NB1: we use the square brackets to select a part of vector_with_na[] # NB2: is.na() searches for NA values # NB3: the exclamation mark ! in front of is.na() means &quot;not&quot; # !is.na() searches for non NA values (of the vector vector_with_na) vector_without_na &lt;- vector_with_na[!is.na(vector_with_na)] mean(vector_without_na) ## [1] 5.5 # Use the na.omit() function to remove NA values from a vector vector_without_na &lt;- na.omit(vector_with_na) median(vector_without_na) ## [1] 5.5 # Use the na.rm option within the sum() function to ignore the NA values sum(vector_with_na, na.rm = TRUE) ## [1] 55 More complicated data Usually, our datasets are more complicated than just a vector. Lists Many datasets in R are stored as lists. A list is a collection of vectors and lists of any length and data type. Lists can contain: Single values of different types (single values are actually vectors with 1 element). A combination of vectors each with a different data type and length. Other lists (which may contain vectors and other lists). Any combination of the above. Lists provide a way of storing hierarchical complex data (like the folder/subfolder structure on your computer with combinations of different file types and the number of files in each folder/subfolder). A list is created with the function list(). The structure of a list can be checked by the function str(). To check if an object is a list use is.list() or class(). Example # Create a list composed of a numeric, character, vector, and logical element list_1 &lt;- list(1, &quot;bert&quot;, c( 1:10 ), TRUE) class(list_1) ## [1] &quot;list&quot; is.list(list_1) ## [1] TRUE # Show for each element of the list the type of data str(list_1) ## List of 4 ## $ : num 1 ## $ : chr &quot;bert&quot; ## $ : int [1:10] 1 2 3 4 5 6 7 8 9 10 ## $ : logi TRUE # Add object list_1 as an element to list_2 list_2 &lt;- list(2, &quot;ernie&quot;, c( 11:20 ), FALSE, list_1) str(list_2) ## List of 5 ## $ : num 2 ## $ : chr &quot;ernie&quot; ## $ : int [1:10] 11 12 13 14 15 16 17 18 19 20 ## $ : logi FALSE ## $ :List of 4 ## ..$ : num 1 ## ..$ : chr &quot;bert&quot; ## ..$ : int [1:10] 1 2 3 4 5 6 7 8 9 10 ## ..$ : logi TRUE Selecting list elements To select element(s) of an R list we use square brackets for indexing: [index]. Element(s) that are subtracted from a list using the square brackets are returned as a separate list. Example # Select 2 elements (elements 1 and 2) from list_2 list_2_element_1_2 &lt;- list_2[c( 1,2 )] str(list_2_element_1_2) ## List of 2 ## $ : num 2 ## $ : chr &quot;ernie&quot; typeof(list_2_element_1_2) ## [1] &quot;list&quot; To select element(s) of an R list we can also use double square brackets for indexing: [[index]]. Element(s) that are subtracted from a list using double square brackets are not returned as a separate list. In other words the returned element is not part of a list anymore. This way we can easily substract elements of an element from a list (to get deeper in the hierarchical data structure). Example # Use single square brackets [index] list_2_element3 &lt;- list_2[3] typeof(list_2_element3) ## [1] &quot;list&quot; # Use double square brackets [[index]] list_2_element3 &lt;-list_2[[ 3 ]] typeof(list_2_element3) ## [1] &quot;integer&quot; list_2[[3]][5] ## [1] 15 Exercise 1 Write R code to create a list containing the following 5 elements: a vector containing the sequence 1 to 360 with stepsize 1; a vector containing the alphabet in capital letters; a vector containing 2pi / divided by each element of the sequence 1 to 360 with stepsize 1; a vector containing the first 20 numbers of the exponential sequence 2n (2, 4, 8, 16 and so on); and a list containing the following 3 elements: a vector containing “Hello,”; a vector containing “World”; and a vector containing the log10 values of the sequence 1 to 100 with stepsize 1. Click for the answer list_4.1 &lt;-list(1:360, LETTERS, 2*pi/1:360, 2^(1:20), list(&quot;Hello,&quot;, &quot;World&quot;, log10(1:100) ) ) Exercise 1 Extract the value of 2^10 from the list of the previous exercise using indices. Extract the value of log10(99) from the list of the previous exercise using indices. Click for the answer list_4.1[[4]][10] ## [1] 1024 list_4.1[[5]][[3]][99] ## [1] 1.995635 Headers It is good practice to name the different elements of a list with headers. The headers are very useful in referring to the different elements in the list by using indexing as shown for the vectors. Example # Create a list with named elements &quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot; and &quot;e&quot; list_3 &lt;- list(a = 2, b = &quot;ernie&quot;, c = c(11:20), d = FALSE, e = list_1) # Extract an element by refering to the name of the element list_3[[&quot;a&quot;]] ## [1] 2 # Check if elements of a list have names names(list_3) ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; # Change the name of the 4th element of list_3 to &quot;pino&quot; names(list_3)[4] &lt;- &quot;pino&quot; names(list_3) ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;pino&quot; &quot;e&quot; An alternative way of extracting elements of a “named” list is to directly refer to the name of an element using the $ sign. It works similar as the double square brackets [[index]] but without the quotes. Example # Use the $ sign to extract a named element from a list list_3$a ## [1] 2 list_3$e[[3]][10] ## [1] 10 Dataframes Experimental data is often organized into a dataframe: rows represent all the observations of a single unique “entity”. Each column represents a variable and the data values of each column are stored as a vector of equal length (in contrast to the list structure which can contain vectors or list of any length). You will have encoutered this 2-dimensional format before when using Excel or SPSS. Separate columns can store different kinds of values such as numeric or characters, but within a column the values are of the same type. Below you can see an example of a dataframe: sample treatment weight blood_pressure cholesterol human1 control 80 80/120 20 human2 control 82 85/110 25 human3 control 78 78/115 32 human4 50 ng/mg 76 90/125 45 human5 50 ng/mg 83 92/120 43 human6 50 ng/mg 81 87/119 NA In R, several functions for dataframes are available: data.frame() is used to create a dataframe. str() is used to check the structure of a dataframe. You can check if an R object is a dataframe using is.data.frame() or class(). dim() can be used to check the dimensions of the dataframe. names() can be used to check to column names (names of the variables). View() can be used to see the contents of a dataframe as a spreadsheet in R studio. You can see the first part of the dataframe using head(); similarly, you can see the last part of the dataframe using tail(). Example df1 &lt;-data.frame(var1 = 1:5, var2 = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;), var3 = c(rep(&quot;bert&quot;, 3) , rep(&quot;ernie&quot;, 2))) df1 ## var1 var2 var3 ## 1 1 a bert ## 2 2 b bert ## 3 3 c bert ## 4 4 d ernie ## 5 5 e ernie # R has some built-in datasets, like mtcars, which are dataframes mtcars ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 class(mtcars) ## [1] &quot;data.frame&quot; # Check the first 6 lines of the dataframe head(mtcars) Data analysis using dataframes After the initial inspection of the dataframe, we start with the actual data analysis and ask interesting questions. For example, for the mtcars dataset, we can ask the following questions: What is the average of the mpg? What is the min / max value of the hp? What is the correlation between hp and mpg? What is the difference in mpg between cars with different gears (using a boxplot)? We can also calculate for each variable in the dataset a summary (see example below). Example # Use the $ sign to select a variable mean(mtcars$mpg) ## [1] 20.09062 max(mtcars$hp) ## [1] 335 # Load the package tidyverse to use the ggplot() function # (NB: we will explain ggplot() in the next lessons) library(tidyverse) mtcars %&gt;% ggplot(aes(x = hp, y = mpg)) + geom_point() + geom_smooth(se = FALSE) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; summary(mtcars) ## mpg cyl disp hp drat wt ## Min. :10.40 Min. :4.000 Min. : 71.1 Min. : 52.0 Min. :2.760 Min. :1.513 ## 1st Qu.:15.43 1st Qu.:4.000 1st Qu.:120.8 1st Qu.: 96.5 1st Qu.:3.080 1st Qu.:2.581 ## Median :19.20 Median :6.000 Median :196.3 Median :123.0 Median :3.695 Median :3.325 ## Mean :20.09 Mean :6.188 Mean :230.7 Mean :146.7 Mean :3.597 Mean :3.217 ## 3rd Qu.:22.80 3rd Qu.:8.000 3rd Qu.:326.0 3rd Qu.:180.0 3rd Qu.:3.920 3rd Qu.:3.610 ## Max. :33.90 Max. :8.000 Max. :472.0 Max. :335.0 Max. :4.930 Max. :5.424 ## qsec vs am gear carb ## Min. :14.50 Min. :0.0000 Min. :0.0000 Min. :3.000 Min. :1.000 ## 1st Qu.:16.89 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.:3.000 1st Qu.:2.000 ## Median :17.71 Median :0.0000 Median :0.0000 Median :4.000 Median :2.000 ## Mean :17.85 Mean :0.4375 Mean :0.4062 Mean :3.688 Mean :2.812 ## 3rd Qu.:18.90 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:4.000 3rd Qu.:4.000 ## Max. :22.90 Max. :1.0000 Max. :1.0000 Max. :5.000 Max. :8.000 Tibbles Dataframes are part of base R and have some typical behaviors which are not always useful. During this course, we will extensively work with the tidyverse package. To make dataframes compatible with the tidyverse package, we have to change an R base dataframe to a tibble. A tibble is also a dataframe but with different behavior. There are several useful functions when working with tibbles: tibble() can be used to create tibbles from scratch. as_tibble() can be used to convert an existing dataframe to a tibble. By default, the function removes rownames, if they were present. Tibbles do not support rownames: all values, including rownames, are stored in vectors of equal length as variables. You can check if an object is a tibble using is_tibble() or class(). Example library(tidyverse) # Create a new tibble object with three columns tibble_1 &lt;- tibble(number = 1:10, letters = letters[1:10], random = runif(10)) is_tibble(tibble_1) ## [1] TRUE # Show Rstudio spreadsheet of the mtcars dataframe including rows View(mtcars) # Check the data structure of the object class(mtcars) ## [1] &quot;data.frame&quot; # Convert an existing R dataframe to a tibble tibble_2 &lt;- as_tibble(mtcars) class(tibble_2) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; # Show first rows of the tibble structure tibble_2 ## # A tibble: 32 × 11 ## mpg cyl disp hp drat wt qsec vs am gear carb ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 21 6 160 110 3.9 2.62 16.5 0 1 4 4 ## 2 21 6 160 110 3.9 2.88 17.0 0 1 4 4 ## 3 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 ## 4 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 ## 5 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 ## 6 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 ## 7 14.3 8 360 245 3.21 3.57 15.8 0 0 3 4 ## 8 24.4 4 147. 62 3.69 3.19 20 1 0 4 2 ## 9 22.8 4 141. 95 3.92 3.15 22.9 1 0 4 2 ## 10 19.2 6 168. 123 3.92 3.44 18.3 1 0 4 4 ## # … with 22 more rows # select a specific variable of the tibble with the $ sign tibble_2$gear ## [1] 4 4 4 3 3 3 3 4 4 4 4 3 3 3 3 3 3 4 4 4 3 3 3 3 3 4 5 5 5 5 5 4 It is possible for a tibble to have column names that are not valid R variable names, a.k.a. non-syntactic names. For example, they might not start with a letter, or they might contain unusual characters like a space. To refer to these variables, you need to surround them with backticks: `. You will use it later in the course. Example weird_column_names &lt;- tibble(`0` = 1:10, `@` = seq(102, 120, 2), `&amp;*(` = letters[1:10]) colnames(weird_column_names) ## [1] &quot;0&quot; &quot;@&quot; &quot;&amp;*(&quot; weird_column_names$`&amp;*(` ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; Factors Experimental research involves “entities” that are measured. What is being measured (the variable) depends on the research question: what do we want to know of the “entity”? Many variables are continuous variables: the value is obtained by measurement and can be any value within a certain range (e.g. blood pressure, weight,concentration of bacterial toxins etc.). Variables can also be categorical. Categorical variables take on values (levels) that are names. The names have a fixed number of possibilities such as gender (male, female), political party, type of car, color. When dealing with categorical data, you could use vectors with “character” elements to describe the different levels of categorical data, like you would for example in Excel. However in R, factors can be used to describe the different levels of categorical data. To work with factors, we will use the forcats package (which is regrettably not about cats but about categorical data): # first load the package (and tidyverse too for good measure) library(tidyverse) library(forcats) With factors, we can: control the sorting behavior of the categorical levels; modify categorical levels; and prevent typos in the categorical levels. Let’s demonstrate the behavior of factors with an example. A researcher wants to know which month has the highest flu incidence rate in the Netherlands and counts for each month the number of registered people with flu. flu_month &lt;- tibble(patient = paste0(c(&quot;patient&quot;), 1:16), month = c(&quot;jan&quot;, &quot;jam&quot;, &quot;mar&quot;, &quot;nov&quot;, &quot;nov&quot;, &quot;dec&quot;, &quot;feb&quot;, &quot;apr&quot;, &quot;jun&quot;, &quot;sept&quot;, &quot;dec&quot;, &quot;feb&quot;, &quot;aug&quot;, &quot;may&quot;, &quot;jul&quot;, &quot;sep&quot;)) flu_month ## # A tibble: 16 × 2 ## patient month ## &lt;chr&gt; &lt;chr&gt; ## 1 patient1 jan ## 2 patient2 jam ## 3 patient3 mar ## 4 patient4 nov ## 5 patient5 nov ## 6 patient6 dec ## 7 patient7 feb ## 8 patient8 apr ## 9 patient9 jun ## 10 patient10 sept ## 11 patient11 dec ## 12 patient12 feb ## 13 patient13 aug ## 14 patient14 may ## 15 patient15 jul ## 16 patient16 sep There are three problems with this dataset: If we want to summarize the data, the variable “month” is ordered alphabetically. One month is misspelled as “jam”. One month is spelled as “sep” or “sept”. Let’s summarise the data with the function count(). flu_month %&gt;% count(month) ## # A tibble: 13 × 2 ## month n ## &lt;chr&gt; &lt;int&gt; ## 1 apr 1 ## 2 aug 1 ## 3 dec 2 ## 4 feb 2 ## 5 jam 1 ## 6 jan 1 ## 7 jul 1 ## 8 jun 1 ## 9 mar 1 ## 10 may 1 ## 11 nov 2 ## 12 sep 1 ## 13 sept 1 The count() function returns the month in alphabetical order which is not useful. Also, there are two levels for January: “jam” and “jan” each with a value. In addition, there are two levels for September: “sep” and “sept” each with a value. To prevent these issues, we can create a factor data structure. On forehand, levels of the categorical variable are defined with a character vector. To view the values in the dataset that are not part of the predefined level list, we can use the function parse_factor(). This function comes with two arguments: a character vector to be analyzed; and the levels of the categorical value. # Create a vector containing the level definition month_levels &lt;- c(&quot;jan&quot;, &quot;feb&quot;, &quot;mar&quot;, &quot;apr&quot;, &quot;may&quot; ,&quot;jun&quot;, &quot;jul&quot;, &quot;aug&quot;, &quot;sep&quot;, &quot;oct&quot;, &quot;nov&quot;, &quot;dec&quot;) parse_factor(flu_month$month, levels = month_levels) ## Warning: 2 parsing failures. ## row col expected actual ## 2 -- value in level set jam ## 10 -- value in level set sept ## [1] jan &lt;NA&gt; mar nov nov dec feb apr jun &lt;NA&gt; dec feb aug may jul sep ## attr(,&quot;problems&quot;) ## # A tibble: 2 × 4 ## row col expected actual ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2 NA value in level set jam ## 2 10 NA value in level set sept ## Levels: jan feb mar apr may jun jul aug sep oct nov dec # Correct the misspellings flu_month$month[flu_month$month==&quot;jam&quot;] &lt;- c(&quot;jan&quot;) flu_month$month[flu_month$month==&quot;sept&quot;] &lt;- c(&quot;sep&quot;) We can now convert the character variable “month” to a factor variable: # Convert the variable &#39;month&#39; to a factor flu_month$month &lt;- factor(flu_month$month, levels = month_levels) flu_month ## # A tibble: 16 × 2 ## patient month ## &lt;chr&gt; &lt;fct&gt; ## 1 patient1 jan ## 2 patient2 jan ## 3 patient3 mar ## 4 patient4 nov ## 5 patient5 nov ## 6 patient6 dec ## 7 patient7 feb ## 8 patient8 apr ## 9 patient9 jun ## 10 patient10 sep ## 11 patient11 dec ## 12 patient12 feb ## 13 patient13 aug ## 14 patient14 may ## 15 patient15 jul ## 16 patient16 sep To view the levels of a “factor” variable use the function levels(): levels(flu_month$month) ## [1] &quot;jan&quot; &quot;feb&quot; &quot;mar&quot; &quot;apr&quot; &quot;may&quot; &quot;jun&quot; &quot;jul&quot; &quot;aug&quot; &quot;sep&quot; &quot;oct&quot; &quot;nov&quot; &quot;dec&quot; Let’s summarise the data again with the function count(): flu_month %&gt;% count(month) ## # A tibble: 11 × 2 ## month n ## &lt;fct&gt; &lt;int&gt; ## 1 jan 2 ## 2 feb 2 ## 3 mar 1 ## 4 apr 1 ## 5 may 1 ## 6 jun 1 ## 7 jul 1 ## 8 aug 1 ## 9 sep 2 ## 10 nov 2 ## 11 dec 2 The months are now in the right order. Note that October is omitted because no values were recorded for this month. using functions: drc We will use the drc package for the following exercises. Packages contain functions and data, and can be installed using the command line (or click tools –&gt; install packages): Type and run in the console to install and load the drc package: install.packages(&quot;drc&quot;) library(drc) This package contains functions and data for the analysis of dose-response curves. For instance, it contains a dataset of part of a study to investigate the joint action of phenolic acids on root growth inhibition of perennial ryegrass (Lolium perenne L). Look at the data by typing ryegrass in the console. You will see a dataset with 24 measurements and 2 variables. rootl is the root length of perennial ryegrass measured in cm., conc is the concentration of ferulic acid is in mM. Exercise 1 Using a function, calculate the avergae root length of the measured plants. Click for the answer library(drc) mean(ryegrass$rootl) ## [1] 4.327204 With a few lines of code, we can ask R to fit a 4-parameter log-logistic model to the data, and give us a dose response curve. We use the function drm() (Dose Response Model), which takes in this case 3 inputs: formula : which effect are we interested in? data: which data to use fct: which function to fit model&lt;- drm(formula = rootl~conc, # we are interested in rootlength as a function of (~) concentration data=ryegrass, # input fct=LL.4(names = c(&quot;Slope&quot;, &quot;Lower Limit&quot;, &quot;Upper Limit&quot;, &quot;ED50&quot;))) #LL.4 means 4-parameter log-logistic summary(model) ## ## Model fitted: Log-logistic (ED50 as parameter) (4 parms) ## ## Parameter estimates: ## ## Estimate Std. Error t-value p-value ## Slope:(Intercept) 2.98222 0.46506 6.4125 2.960e-06 *** ## Lower Limit:(Intercept) 0.48141 0.21219 2.2688 0.03451 * ## Upper Limit:(Intercept) 7.79296 0.18857 41.3272 &lt; 2.2e-16 *** ## ED50:(Intercept) 3.05795 0.18573 16.4644 4.268e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: ## ## 0.5196256 (20 degrees of freedom) And we can plot our model by using the plot() function. The plot will appear in the lower right of your screen. plot(model, type=&quot;all&quot;) You can also calculate the EC10, EC20, EC50 etc with the ED() function. (type ?ED in the console to see why we set the type of confidence interval to “delta”. ) ED(model, c(10,20,50), interval=&quot;delta&quot;) ## ## Estimated effective doses ## ## Estimate Std. Error Lower Upper ## e:1:10 1.46371 0.18677 1.07411 1.85330 ## e:1:20 1.92109 0.17774 1.55032 2.29186 ## e:1:50 3.05795 0.18573 2.67053 3.44538 Of course, you can compare different models (see getMeanFunctions() for a complete list). for instance: # W2.3: a weibull type 2 function with 4 parameters model.W24 &lt;- drm(rootl~conc, data=toxdata, fct=W2.4(names = c(&quot;Slope&quot;, &quot;Lower Limit&quot;, &quot;Upper Limit&quot;, &quot;ED50&quot;))) # a Log logistic model with 3 parameters # (3 parameters means fixing the lower limit at zero. Which makes sense as rootlength can&#39;t be negative) model.LL3 &lt;- drm(rootl~conc, data=ryegrass, fct=LL.3(names = c(&quot;Slope&quot;, &quot;Upper Limit&quot;, &quot;ED50&quot;))) plot(model.LL4, broken = TRUE, xlab=&quot;Concentration&quot;, ylab=&quot;Percent Response&quot;, type=&#39;all&#39;,lty=1, lwd=2) plot(model.W24, add=TRUE,col=&quot;orange&quot;,lty=1, lwd=2) You can calculate the Akaike’s information criterion and the residual variance (smaller = better) with the mselect() function, to make an informed decision between different models: mselect(model.LL3, fctList = list(W2.4()), linreg=TRUE) ## logLik IC Lack of fit Res var ## W2.4 -15.91352 41.82703 0.9450713 0.2646283 ## LL.3 -18.60413 45.20827 0.3531679 0.3153724 ## Cubic -25.53428 61.06856 NA 0.5899609 ## Quad -35.11558 78.23116 NA 1.2485122 ## Lin -50.47554 106.95109 NA 4.2863247 scripts and Rmarkdown So far, we have run a lot of code in the console. Or perhaps you used a script to save some commands. Usually though, you will be working on a specific dataset, rather than playing around with course material. To save all the steps for a specific analysis, you can save your code in a script, or in a markdown file. scripts Scripts have been introduced before. Quite simply, they are just all R code, but in a tekst file so you can save your code for later (and importantly: for reproducability). All regular tekst in a .r script will be regarded as R code. Comments can be added by putting a hashtag in front of the tekst. Any plots will be displayed in the window at the lower right of the screen. Exercise 1 If you haven’t already, make a new script with the code for fitting dose response curves we used in the previous examples. Save you script on your computer. Click for the answer click file –&gt; new file –&gt; R script (or click the empty paper with green + in the upper left corner) copy paste your code from the examples to your script. run the script (upper right corner, press Run) to check if it is still working. save the script to your computer. library(drc) library(tidyverse) # fit a 4-parameter log-logistic model model&lt;- drm(formula = rootl~conc, # we are interested in rootlength as a function of (~) concentration data=ryegrass, # input fct=LL.4(names = c(&quot;Slope&quot;, &quot;Lower Limit&quot;, &quot;Upper Limit&quot;, &quot;ED50&quot;))) #LL.4 means 4-parameter log-logistic # give me the estimates summary(model) # plot the dose response curve plot(model, type=&quot;all&quot;) # calculate ED10, 20 and 50 ED(model, c(10,20,50), interval=&quot;delta&quot;) # compare 2 different models model.W24 &lt;- drm(rootl~conc, data=toxdata, fct=W2.4(names = c(&quot;Slope&quot;, &quot;Lower Limit&quot;, &quot;Upper Limit&quot;, &quot;ED50&quot;))) # W2.3: a weibull type 2 function with 4 parameters model.LL3 &lt;- drm(rootl~conc, data=ryegrass, fct=LL.3(names = c(&quot;Slope&quot;, &quot;Upper Limit&quot;, &quot;ED50&quot;))) # a Log logistic model with 3 parameters # plot the comparison plot(model.LL4, broken = TRUE, xlab=&quot;Concentration&quot;, ylab=&quot;Percent Response&quot;, type=&#39;all&#39;,lty=1, lwd=2) plot(model.W24, add=TRUE,col=&quot;orange&quot;,lty=1, lwd=2) # calculate IC mselect(model.LL3, fctList = list(W2.4()), linreg=TRUE) Rmarkdown If you want to present your findings you can either share your code and ask fellow researchers to reproduce your results or alternatively only show the output of your results. Both have their downsides, and Rmarkdown tries to comine the best of both methods. It is a reproducible manner of building documents (such as .pdf, .html and even .docx) which directly includes your code. So when you use code to generate a plot, it is included in the Rmarkdown-file you use to write the paper. It containis both code (method) and output. RMarkdown can be used in three ways: to communicate only the output and conclusions of your analysis to decision makers (and not the code behind the conclusions); to communicate the output and conclusions, as well as the code behind the conclusions to fellow scientists; as an environment in which to do data science, as a modern day lab notebook where you can capture not only what you did, but also what you were thinking. Although here we focus on R code, RMarkdown documents can also integrate Python, Bash, SQL and other (programming) languages into one document showing both code and output. RMarkdown is also frequently used to produce (e-)books. And also, this reader. Let’s start with a simple RMarkdown document to examplify the concept of RMarkdown: To open an existing RMarkdown file, perform the following steps: Click on file &gt; Open File. Navigate to the folder you downloaded the course data to and select file lesson1_example1.Rmd (note: RMarkdown files have the extension .Rmd). Open RMarkdown file lesson1_example1.Rmd. The contents of the file will be displayed in the upper left panel in RStudio and contains the following lines of code: An R markdown file consist of different sections of code each marked with different characters: The first header section starts and ends with — Sections containing code (chunks) start and end with ``` Plain text does not have marking characters Code within plain text starts and ends with ` To render an RMarkdown file (meaning to make an output file) simply click on the knit button located just above the RMarkdown file (see figure above). The output file will be shown in a separate RStudio panel. Knit the RMarkdown file and take a close look at the output file. The output document is saved in the same folder as the original RMarkdown file with the extension ‘.html’. This is the default output format but can be changed to, for example, a PDF or Word format. IMPORTANT: by default the output file contains the code itself, messages and the output of the code. The default behavior of what to show in the output file can be changed and will be discussed later in this lesson. But first we will explain the lay-out of the RMarkdown file. bla YAML header The first section of the RMarkdown example is the YAML header. The YAML header provides general information of the output document: title: can be filled in when creating a new RMarkdown file. author: can be filled in when creating a new RMarkdown file. date: automatically set when creating a new RMarkdown file. output: by default RMarkdown creates an output file in HTML format. If you want another output format change output: to another file format such as PDF or Word (although you need additional packages). If you want to make different RMarkdown output files from an existing RMarkdown, it is advisable to set the date in the YAML header as follows: &#39;`r Sys.Date()`&#39; This R function sets the current date. Note that the function is between a pair of single quotes and backticks (see also later in this lesson). The YAML header provides many other options to control “whole document” settings. However, this is beyond the scope of this course. Example --- title: &quot;Diamond sizes&quot; author: &quot;Your Name&quot; date: 2019-10-31 output: html_document --- ```{r setup} library(ggplot2) library(dplyr) smaller &lt;- diamonds %&gt;% filter(carat &lt;= 2.5) ``` We have data about ` r nrow( diamonds ) ` diamonds. Only ` r nrow( diamonds ) - nrow( smaller ) ` are larger than 2.5 carats. The distribution of the remainder is shown below: ```{r frequency} smaller %&gt;% ggplot(aes(carat)) + geom_freqpoly(binwidth = 0.01) ``` Code chunks The RMarkdown example above contains two chunks. A chunk is a section in the RMarkdown file containing the actual code. A chunk of code starts and ends with three backtick marks. To define the beginning of the code the three backtick marks are minimally followed with {r}. The r signals that the code is R. Remember that we can also use code of other languages such as bash or python. In that case the chunk header will be {bash} or {python}, respectively. Within the {r} header, you can optionally provide a name of the header. In the example above, the first header is named “setup” and the second header is named “frequency”. Naming chunks has the advantage that your RMarkdown becomes organized so that is will be easy to search for and navigate to a specific chunk. To search for a chunk, click on search button below the RMarkdown file (see figure below). Another use of named chunks is to save figures. If a chunk creates a figure, the figure can also be saved separately on your server. This option will be discussed in the next section. Output control When rendering an RMarkdown file, by default the R code in a chunk is executed and the output file will contain the code, messages and output of the code. Most often this is not what you want. (NB: plain text is always printed.) To control what will be shown in the output file we will use chunk options. If you want to apply a similar set of options for all “chunks”, you can use knitr::opts$set as shown below. Put this chunk, named ‘setup’ at the beginning of your RMarkdown file. The chunck options defined here will be generally applied to the whole document: ```{r setup} knitr::opts_chunk$set(option1 = &#39;hide&#39;, option2 = FALSE, option3 = FALSE ) ``` It is also possible to set options for a single chunk. Simply put the options in the header of the chunk you want to modify, as shown below. General chunk options as shown above will be overwritten by the single chunk options. ```{r setup, option1 = FALSE, option2 = FALSE} library(ggplot2) library(dplyr) smaller &lt;- diamonds %&gt;% filter(carat &lt;= 2.5) ``` There are more than 60 options to add to a chunk header. Some of these options determine what happens with the chunk code: eval = FALSE: the code is not executed. The chunk code is shown in the output file. This is useful for displaying example code. include = FALSE: the code is executed, but the chunk code and all of its output (results, figures, messages and warnings) are not shown in the output file. This is useful for chunks performing data wrangling, for example data filtering or creating new variables, which are later used in graphs or summary tables. echo = FALSE: the code is executed, but the chunk code is not in the output file. All output of the chunk is shown in the output file. This is useful for showing the final results (graphs) of your data analysis and not how you achieved your results. Use this when writing reports aimed at people who do not want to see the underlying R code. Note that the option “include = FALSE” supresses all output of a chunk. To specifically control what should not be be in the output file you can make use of the following options: results = “hide”: the code is executed and the chunk code is shown in the output file. The results of R functions are not shown in the output file. fig.show = “hide”: the code is executed and he chunk code is shown in the output file. Figures of R functions are not shown in the output file. message = FALSE: the code is executed and the chunk code is shown in the output file. R messages are not shown in the output file. warning = FALSE: the code is executed and the chunk code is shown in the output file. R warnings are not shown in the output file. Table with different chunk output options. yes means present in the output file. Option Run code Show code Output Plots Message Warning eval = FALSE no yes no no no no include = FALSE yes no no no no no echo = FALSE yes no yes yes yes yes results = “hide” yes yes no yes yes yes fig.show = “hide” yes yes yes no yes yes message = FALSE yes yes yes yes no yes warning = FALSE yes yes yes yes yes no Exercise 1 Modify RMarkdown file lesson1_example1.Rmd to only show the figure in the output file. Modify RMarkdown file lesson1_example1.Rmd to only show the chunk code in the output file (not the output of the chunks). Save the Rmd files as lesson1_exercise_1a.Rmd and lesson1_exercise_1b.Rmd respectively in the server folder daur/rmarkdown. Click for the answer {r setup, echo = FALSE, message = FALSE} {r frequency, echo = FALSE} {r setup, message = FALSE} {r frequency, fig.show = &quot;hide&quot;} There are also several options for dealing with figure output and layout: fig.width = n and fig.height = n (where n is a number representing inches): to change the width and height of the figures in your output file. fig.cap = “name of the caption”: to add a caption to the figure plot (a caption is a numbered label, such as “Figure 1”). fig.path = “figures/”: to create separate files of figures created by a chunk. First create a figures folder in the rmarkdown directory. The file containing the figure will be stored in this folder with a similar name as the chunk name. Exercise 1 Modify RMarkdown file lesson1_example1.Rmd to: Show only the figure in the output file. Modify the figure width to 6 and the height half of the width. The figure in the output file has an caption. Modify RMarkdown file lesson1_example1.Rmd to: Show only the plain text. Save the figure in a separate file stored in the server folder daur/rmarkdown/figures. Save the Rmd files as lesson1_exercise_2a.Rmd and lesson1_exercise_2b.Rmd respectively in the server folder daur/rmarkdown Click for the answer {r setup, echo = FALSE, message = FALSE} {r frequency, echo = FALSE, fig.height = 3, fig.width = 6, fig.cap = &quot;Figure1: Distribution of carat values&quot;} {r setup, include=FALSE} {r frequency, include=FALSE, fig.path =&quot;figures/&quot;} In the RMarkdown output file, data frames (tibbles) are printed in the same way as you would see them in the RStudio console. To change this behavior, you can use the knitr::kable() function as demonstrated in RMarkdown file lesson1_flights_table.Rmd. This file is present in server folder daur/rmarkdown/. Open and knit this RMarkdown file (the code is also shown below). Study the code and the RMarkdown output format. Example --- title: &quot;Lesson8_flights_table&quot; author: &quot;Chris van Oevelen&quot; date: &quot;3-11-2019&quot; output: html_document --- ```{r setup} library(nycflights13) library(tidyverse) flights_select &lt;- flights %&gt;% select(year, month, day, carrier, flight, tailnum) ``` ```{r table1} head(flights_select) ``` ```{r table2} knitr::kable( head(flights_select), caption = &quot;Table1: a table with a different lay-out.&quot; ) ``` Exercise 1 For this exercise we will make use of the gapminder dataset from the dslabs package. Select for each year the country with the highest infant_mortality. Present the data in a clear table only showing the variables “country”, “year”, and “infant_mortality”. Create a RMarkdown document containing the R code. The output file of the RMarkdown document should only contain the table. Save the RMarkdown file in the server folder daur/rmarkdown as lesson1_exercise_3.Rmd. Click for the answer --- title: &quot;gapminder_table&quot; author: &quot;Your name&quot; date: &#39;`r Sys.Date()`&#39; output: html_document --- ```{r setup, include=FALSE} library(tidyverse) library(dslabs) ``` ```{r table, warning=FALSE, echo=FALSE} knitr::kable( gapminder %&gt;% select(country, year, infant_mortality) %&gt;% group_by(year) %&gt;% filter(infant_mortality == max(infant_mortality, na.rm = TRUE)), caption = &quot;Table1: countries with highest infant mortality rate from 1960-2016.&quot; ) ``` Plain text formatting An RMarkdown document contains a YAML header and chunks of code to be executed. In addition, plain text can be added to describe your analysis, results or whatever you think is useful to understand your RMarkdown document. Plain text will always be shown in the output file (unless you put it in a chunk as comments, starting with a # sign). The output of plain text can easily be formatted by a set of simple formatting rules: to print plain text in italics surround the text with 1 underscore: _text_ will become text. to print plain text in bold surround the text with 2 underscores: __text__ will become text. to print superscript use ^2^: 10^2^ will become 102. to print subscript use ~2~: H~2~O will become H2O. To print headers use the pound sign in front of the header text: # 1st Level Header ## 2nd Level Header ### 3rd Level Header A quick reference guide can be found in RStudio: Help &gt; Markdown Quick Reference. A complete reference guide can be found in RStudio: Help &gt; Cheatsheets &gt; R markdown Reference Guide. Within plain text, R code can be executed. To incorporate R code within plain text, use r code surrounded by backticks `. The code will be evaluated and printed in the output document. Note that the original code will not be shown in the output document! Exercise 1 Modify RMarkdown document lesson1_exercise_3.Rmd to include: Plain text describing the source of the dataset. Write the package name in italics and the name of the dataset in bold. the sentence: “the infant mortality has dropped from ’ [R code showing the max infant_mortality in 1960] ’ in 1960 to ’ [R code showing max infant_mortality in 2015] ’ in 2015.” NB: the parts between square brackets is R code you have to include; this R code will be evaluated within the plain text section after rendering the output file. Save the RMarkdown file in the server folder daur/rmarkdown as lesson1_exercise_4.Rmd. Click for the answer --- title: &quot;gapminder_table&quot; author: &quot;Your name&quot; date: &#39;`r Sys.Date()`&#39; output: html_document --- ```{r setup, include=FALSE} library(tidyverse) library(dslabs) ``` source of data: The __gapminder__ dataset from the _dslabs_ package. The infant mortaliy has drop from `r year_1960 &lt;- gapminder %&gt;% filter (year == 1960); max (year_1960$infant_mortality, na.rm = TRUE)` in 1960 to `r year_2015 &lt;-gapminder %&gt;% filter (year == 2015); max (year_2015$infant_mortality, na.rm = TRUE)` in 2015. ```{r table, warning=FALSE, echo=FALSE} knitr::kable( gapminder %&gt;% select(country, year, infant_mortality) %&gt;% group_by(year) %&gt;% filter(infant_mortality == max(infant_mortality, na.rm = TRUE)), caption = &quot;Table1: countries with highest infant mortality rate from 1960-2016.&quot; ) ``` Here package To exemplify the idea of RMarkdown we have used a built-in dataset to show the different display options of code and output. However, most often you will refer to datasets or scripts in your RMarkdown file that are stored on the server. This will cause a problem because RMarkdown and knit will not search for files in the project folder but instead in the folder were the RMarkdown file is stored. To solve this problem we will make use of the “here” package. In the beginning of this course we have created an Rproject and before each session you should open the Rproject you are working on. Within the project folder (in this case daur) you organize your data files, scripts files, RMarkdown files and output files in different folders (see figure below). By using an Rproject you are guaranteed that when executing R code / scripts from the server folder scripts, all of your (data) files are read in properly. This is because by definition, within your Rproject environment, R will set the working directory to the project folder and start looking for folders and files relative from the Rproject folder (the folder that contains the .Rproj file, in this case the daur folder). Figure 5: Data organisation on the server. Always create an Rproject to do data analysis. When using RMarkdown, the default behavior is to only look in the folder where the RMarkdown file is stored. This will cause errors when knitting an RMarkdown file because RMarkdown can not find the data files stored in the data folder outside the RMarkdown folder. For example, you source (execute) a script from the scripts folder and this script imports a dataset present in the data folder. Because you work within your Rproject, R will start looking for folders and files relative to the project folder (in this case the daur folder). Contrary, if you are knitting an RMarkdown file and this RMarkdown imports a dataset present in the data folder, RMarkdown will only start looking in the RMarkdown folder and will never find the data folder. This will cause an error and the knitting of the RMarkdown document stops. Example Open via Rstudio the RMarkdown file lesson1_here_example1.Rmd present in server folder daur/rmarkdown. The file contains the following RMarkdown code: ```{r} library(tidyverse) object1 &lt;- read_csv(&quot;data/lesson3/heights.csv&quot;) head(object1) ``` Click on the knit button to render the output file. You will get the following error message: You notice that the current working directory is set to the rmarkdown folder. RMarkdown start looking for folder “data/lesson2/heights.csv” in this folder but can not find it. It throws an error and exits the knitting process. To change the default behaviour of RMarkdown, we will use the here package. The here package recreates a file path to the Rproject folder. The here package is already installed on the server. Example In Rstudio, open the RMarkdown file lesson1_here_example2.Rmd present in server folder daur/rmarkdown. The markdown file contains the following RMarkdown code: ``` {r} library(tidyverse) library(here) object2 &lt;- read_csv(here(&quot;data/lesson3/heights.csv&quot;)) head(object2) ``` The output file will be displayed in a separate RStudio panel. In addition, an output file named lesson1_here_example2.html is created and saved in the rmarkdown folder. Exercise 1 Use dataset c10_gene_expression.txt from the course data. This file contains gene expression values after 0h, 3h, 12, 24h and 48h (hours) of induction of the c10 celline. After induction, the cell undergoes a massive change in gene expression. Show the genes that become highest upregulated after 48 hours (48h) of induction compared to the start condition (= 0h). Only list genes which have a difference of 8 or more between 48h and 0h. Write an RMarkdown document. The RMarkdown document should contain: YAML header. Plain text describing the source of the dataset. Chunks containing R code to create a table of the highest expressed genes after 48h. The RMarkdown output file only contains the table. Save the RMarkdown file in the server folder daur/rmarkdown as lesson1_exercise_5.Rmd. Click for the answer --- title: &quot;Lesson8_Exercise_5&quot; author: &quot;Your Name&quot; date: &#39; `r Sys.Date() ` &#39; output: html_document --- source of the dataset is __daur/data/lesson8/c10_gene_expression.txt__ ``` {r setup, include=FALSE} library(tidyverse) library(here) ``` ```{r import, include=FALSE} # Read in the data c10_gene_expression &lt;- read_tsv(here(&quot;data/lesson8/c10_gene_expression.txt&quot;)) # Create a column with the difference c10_gene_expression_up8 &lt;- mutate(c10_gene_expression, up = `48h` - `0h`) %&gt;% filter(up &gt;= 8) ``` ```{r table_genes, echo=FALSE} knitr::kable( c10_gene_expression_up8, caption = &quot;Table1: Genes with highest induction of expression after 48 hours&quot; ) ``` Creating a new RMarkdown file To create your own Rmarkdown documents in RStudio, perform the following steps: click File &gt; New File &gt; R Markdown. Fill out the Title and Author which will be displayed in the YAML header (this step is optional). Click on OK. A default template will be opened in RStudio showing the YAML header and instruction on how to use RMarkdown: Remove the instructions but leave the YAML header. Start writing your RMarkdown document. Save your document in a folder called rmarkdown in your server project folder. To render your RMarkdown document, click on the knit button. Exercise 1 Create a new RMarkdown document. In this document, create R chuncks to perform the following tasks: Read in the dataset c10_gene_expression.txt from the course data. Convert this dataset to a tidy dataset. Set the time variable as a factor and set the levels of this factor in a logical order. Create a scatter plot (ggplot) that shows the results for the genes “Slfn4” and “Cd14”. The RMarkdown output file contains the code to make the scatter plot and the plot itself. All other R code used for data transformation and wrangling is not shown in the output file. Save the RMarkdown file as lesson1_exercise_genexp.Rmd. Click for the answer --- title: &quot;Lesson8_Exercise_6&quot; author: &quot;Your Name&quot; date: &#39; `r Sys.Date() ` &#39; output: html_document --- ``` {r setup, include=FALSE} library(tidyverse) library(here) ``` ```{r import, include=FALSE} # Read in the data c10_gene_expression &lt;- read_tsv(here(&quot;data/lesson8/c10_gene_expression.txt&quot;)) # Make data tidy gene_expr_tidy &lt;- c10_gene_expression %&gt;% gather(`0h`, `3h`, `12h`, `24h`, `48h`, key = time, value = expression) %&gt;% mutate(time = as.factor(time)) # Reorder the levels of the time variable levels(gene_expr_tidy$time) &lt;- c(&quot;0h&quot;, &quot;3h&quot;, &quot;12h&quot;, &quot;24h&quot;, &quot;48h&quot;) ``` ```{r figure_genes} gene_expr_tidy %&gt;% filter(gene == &quot;Slfn4&quot; | gene == &quot;Cd14&quot;) %&gt;% ggplot(aes(x = time, y = expression, colour = gene)) + geom_point() ``` Practice in the following week on building Rmarkdown reports with data chunks analysing a certain research question. You can use the build-in datasets in R (type data() to see them) "],["lesson-2---data-types-and-structures-in-r.html", "Lesson 2 - Data types and structures in R Learning objectives Data in R Oefenopdracht (thuis) Overview R functions first two lessons", " Lesson 2 - Data types and structures in R Learning objectives After this lesson: Data in R Yesterday we played around with vectors in R. Vectors are one of the simpelest ways to store information in R. But Exercise 2 Use the built-in dataset gss_cat from the forcats package to answer the following questions. gss_cat is a sample of data from the General Social Survey, which is a long-running US survey conducted by the independent research organization NORC at the University of Chicago. Which variables are factors? What are the levels of the “marital” variable? Click for the answer gss_cat ## # A tibble: 21,483 × 9 ## year marital age race rincome partyid relig denom tvhours ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 2000 Never married 26 White $8000 to 9999 Ind,near rep Protestant Southern … 12 ## 2 2000 Divorced 48 White $8000 to 9999 Not str republican Protestant Baptist-d… NA ## 3 2000 Widowed 67 White Not applicable Independent Protestant No denomi… 2 ## 4 2000 Never married 39 White Not applicable Ind,near rep Orthodox-christian Not appli… 4 ## 5 2000 Divorced 25 White Not applicable Not str democrat None Not appli… 1 ## 6 2000 Married 25 White $20000 - 24999 Strong democrat Protestant Southern … NA ## 7 2000 Never married 36 White $25000 or more Not str republican Christian Not appli… 3 ## 8 2000 Divorced 44 White $7000 to 7999 Ind,near dem Protestant Lutheran-… NA ## 9 2000 Married 44 White $25000 or more Not str democrat Protestant Other 0 ## 10 2000 Married 47 White $25000 or more Strong republican Protestant Southern … 3 ## # … with 21,473 more rows levels(gss_cat$marital) ## [1] &quot;No answer&quot; &quot;Never married&quot; &quot;Separated&quot; &quot;Divorced&quot; &quot;Widowed&quot; &quot;Married&quot; Exercise 2 Run this code in R to create a tibble with categorical data: marital_status &lt;- readRDS(&quot;data/lesson2/lesson2_exercise_8.2.rds&quot;) The levels of this dataset are: “No answer”, “Never married”, “Separated”, “Divorced”, “Widowed” and “Married”. Use this dataset and the levels to answer the following questions. Search for data values that are not in the level list. Correct the misspellings. How many data values are in each of the categorical levels? Click for the answer marital_levels &lt;- c(&quot;No answer&quot;, &quot;Never married&quot;, &quot;Separated&quot;, &quot;Divorced&quot;, &quot;Widowed&quot;, &quot;Married&quot;) parse_factor(marital_status$marital, levels = marital_levels) ## Warning: 3 parsing failures. ## row col expected actual ## 1001 -- value in level set divorced ## 5001 -- value in level set Maried ## 10001 -- value in level set never married ## [1] Never married Divorced Widowed Never married Divorced Married Never married ## [8] Divorced Married Married Married Married Married Married ## [15] Divorced Married Widowed Never married Married Married Married ## [22] Married Never married Widowed Widowed Widowed Widowed Widowed ## [29] Divorced Widowed Widowed Married Married Never married Married ## [36] Never married Never married Never married Never married Never married Married Married ## [43] Divorced Never married Never married Never married Married Married Married ## [50] Married Never married Married Married Married Married Divorced ## [57] Divorced Divorced Never married Never married Married Married Never married ## [64] Divorced Never married Widowed Divorced Married Never married Never married ## [71] Widowed Widowed Widowed Widowed Widowed Never married Widowed ## [78] Never married Married Never married Married Married Widowed Married ## [85] Married Divorced Never married Separated Never married Widowed Widowed ## [92] Married Divorced Never married Never married Never married Married Married ## [99] Widowed Divorced Married Married Married Married Widowed ## [106] Married Divorced Divorced Married Never married Married Married ## [113] Separated Married Never married Divorced Divorced Separated Married ## [120] Widowed Separated Divorced Married Divorced Never married Divorced ## [127] Divorced Married Never married Never married Never married Married Married ## [134] Divorced Widowed Never married Widowed Divorced Separated Widowed ## [141] Never married Widowed Widowed Widowed Widowed Never married Never married ## [148] Never married Never married Never married Never married Never married Married Married ## [155] Separated Never married Widowed Widowed Married Married Married ## [162] Married Married Never married Married Divorced Never married Married ## [169] Married Married Never married Never married Married Married Married ## [176] Married Married Married Married Married Married Widowed ## [183] Widowed Married Married Married Divorced Married Married ## [190] Never married Never married Never married Divorced Never married Never married Married ## [197] Never married Separated Never married Never married Divorced Widowed Separated ## [204] Married Married Separated Divorced Divorced Never married Married ## [211] Married Married Married Married Never married Never married Never married ## [218] Married Married Separated Married Never married Married Never married ## [225] Divorced Never married Never married Never married Married Married Never married ## [232] Married Widowed Married Never married Divorced Married Separated ## [239] Married Married Married Widowed Married Widowed Married ## [246] Never married Married Married Married Widowed Married Married ## [253] Married Divorced Never married Married Married Divorced Married ## [260] Never married Separated Divorced Widowed Married Separated Widowed ## [267] Married Never married Never married Married Married Never married Never married ## [274] Never married Divorced Widowed Separated Divorced Widowed Never married ## [281] Never married Never married Married Married Never married Never married Never married ## [288] Divorced Never married Divorced Married Never married Separated Separated ## [295] Never married Married Never married Divorced Never married Never married Never married ## [302] Never married Divorced Divorced Married Married Married Married ## [309] Married Divorced Married Married Married Never married Married ## [316] Never married Never married Married Married Married Divorced Never married ## [323] Never married Married Married Married Never married Never married Never married ## [330] Married Widowed Married Married Widowed Married Married ## [337] Married Never married Never married Married Married Married Married ## [344] Married Never married Married Married Never married Never married Widowed ## [351] Married Married Married Never married Married Married Never married ## [358] Married Divorced Married Married Separated Never married Never married ## [365] Married Married Divorced Married Married Never married Never married ## [372] Never married Never married Divorced Married Married Divorced Never married ## [379] Never married Married Never married Widowed Married Married Divorced ## [386] Married Married Separated Married Married Married Widowed ## [393] Divorced Married Married Married Divorced Married Divorced ## [400] Never married Married Married Never married Separated Separated Never married ## [407] Divorced Married Married Married Married Married Never married ## [414] Married Married Married Married Never married Married Married ## [421] Married Married Divorced Divorced Married Married Married ## [428] Married Never married Married Never married Married Never married Divorced ## [435] Divorced Divorced Never married Married Separated Divorced Widowed ## [442] Divorced Divorced Never married Divorced Widowed Divorced Divorced ## [449] Divorced Never married Divorced Widowed Widowed Married Married ## [456] Never married Never married Never married Married Married Married Widowed ## [463] Divorced Never married Married Married Married Never married Married ## [470] Married Married Widowed Never married Married Divorced Divorced ## [477] Married Widowed Married Married Married Married Married ## [484] Never married Divorced Married Never married Separated Widowed Divorced ## [491] Married Married Never married Separated Never married Married Married ## [498] Married Married Never married Married Married Never married Divorced ## [505] Married Married Married Never married Never married Never married Never married ## [512] Never married Never married Never married Never married Never married Never married Married ## [519] Separated Married Never married Divorced Married Widowed Married ## [526] Never married Married Married Never married Widowed Never married Never married ## [533] Married Married Separated Never married Never married Married Married ## [540] Married Married Divorced Married Married Married Married ## [547] Married Widowed Separated Separated Never married Widowed Widowed ## [554] Divorced Never married Separated Divorced Never married Never married Never married ## [561] Married Married Never married Married Married Married Divorced ## [568] Married Married Never married Married Married Divorced Never married ## [575] Married Married Married Never married Married Never married Never married ## [582] Married Married Separated Never married Never married Widowed Married ## [589] Married Divorced Married Married Married Married Married ## [596] Married Married Widowed Never married Divorced Married Married ## [603] Married Never married Never married Never married Never married Never married Never married ## [610] Married Married Never married Married Married Married Married ## [617] Never married Separated Married Never married Never married Married Married ## [624] Never married Married Never married Divorced Married Divorced Widowed ## [631] Married Never married Married Never married Separated Divorced Never married ## [638] Widowed Separated Married Never married Married Never married Never married ## [645] Never married Divorced Divorced Never married Never married Married Divorced ## [652] Never married Married Married Widowed Never married Separated Divorced ## [659] Divorced Divorced Never married Married Divorced Married Separated ## [666] Never married Married Married Married Married Married Married ## [673] Married Never married Widowed Never married Never married Divorced Never married ## [680] Married Never married Never married Married Married Married Married ## [687] Married Never married Divorced Separated Divorced Married Married ## [694] Married Married Married Married Married Married Never married ## [701] Divorced Married Divorced Never married Never married Never married Never married ## [708] Widowed Married Married Never married Never married Married Divorced ## [715] Married Married Married Widowed Never married Never married Divorced ## [722] Married Married Married Divorced Divorced Married Widowed ## [729] Never married Widowed Married Widowed Divorced Married Married ## [736] Widowed Widowed Divorced Separated Widowed Married Married ## [743] Married Never married Married Married Married Married Married ## [750] Married Married Married Married Married Married Married ## [757] Never married Never married Married Divorced Married Married Never married ## [764] Never married Widowed Married Widowed Married Married Never married ## [771] Never married Never married Divorced Divorced Never married Divorced Never married ## [778] Never married Never married Married Divorced Married Widowed Married ## [785] Married Never married Married Divorced Divorced Never married Never married ## [792] Divorced Divorced Married Married Never married Married Separated ## [799] Married Married Married Never married Never married Married Married ## [806] Married Never married Widowed Widowed Widowed Widowed Married ## [813] Married Widowed Married Married Never married Widowed Widowed ## [820] Never married Divorced Divorced Married Divorced Separated Married ## [827] Married Widowed Never married Married Never married Married Married ## [834] Never married Married Married Married Widowed Married Widowed ## [841] Married Never married Separated Widowed Married Widowed Never married ## [848] Widowed Widowed Widowed Married Married Widowed Separated ## [855] Married Married Separated Married Widowed Married Divorced ## [862] Never married Widowed Never married Never married Divorced Married Never married ## [869] Married Divorced Married Married Never married Divorced Never married ## [876] Divorced Widowed Divorced Married Divorced Divorced Widowed ## [883] Widowed Never married Married Divorced Married Married Divorced ## [890] Married Widowed Married Married Married Married Married ## [897] Married Married Married Married Never married Married Never married ## [904] Divorced Married Widowed Divorced Divorced Married Divorced ## [911] Married Married Never married Divorced Separated Married Never married ## [918] Divorced Never married Never married Widowed Never married Married Married ## [925] Divorced Separated Separated Married Divorced Divorced Married ## [932] Divorced Married Divorced Never married Divorced Divorced Never married ## [939] Never married Never married Married Widowed Married Married Divorced ## [946] Married Divorced Divorced Married Married Married Married ## [953] Never married Divorced Divorced Never married Separated Never married Married ## [960] Married Married Married Married Never married Married Never married ## [967] Married Divorced Married Married Never married Never married Married ## [974] Divorced Widowed Divorced Never married Married Married Divorced ## [981] Married Never married Never married Never married Separated Never married Divorced ## [988] Married Divorced Married Married Married Married Married ## [995] Married Married Never married Married Married Married ## [ reached getOption(&quot;max.print&quot;) -- omitted 20483 entries ] ## attr(,&quot;problems&quot;) ## # A tibble: 3 × 4 ## row col expected actual ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1001 NA value in level set divorced ## 2 5001 NA value in level set Maried ## 3 10001 NA value in level set never married ## Levels: No answer Never married Separated Divorced Widowed Married marital_status$marital[1001] &lt;- c(&quot;Divorced&quot;) marital_status$marital[5001] &lt;- c(&quot;Married&quot;) marital_status$marital[10001] &lt;- c(&quot;Never married&quot;) marital_status %&gt;% count(marital) ## # A tibble: 6 × 2 ## marital n ## &lt;chr&gt; &lt;int&gt; ## 1 Divorced 3383 ## 2 Married 10117 ## 3 Never married 5416 ## 4 No answer 17 ## 5 Separated 743 ## 6 Widowed 1807 Always think about the variables in your dataset. Are they categorical, continuous or discrete? Let’s say we test different variables in the people in this class: continuous variables: numeric variables that can have any value between any two values. Examples: the hair length (can be anything from 0 to let’s say 400 cm, including for instance 15,36791 cm.), body temperature, pH of a magic potion. discrete variables: also numbers, but these numbers can’t be anything between certain limits. They have discrete options. Typically, this is count data, but not always. For instance: the number of people wearing blue shirts (can be 3, 5 or 8, but can’t be 5,36791 or 2,6), the result of rolling a die. categorical variables: contain a finite number of categories or distinct groups. For instance, eye colour (blue, brown, green, grey, etc), home town (Utrecht, Amsterdam, Zuidlaren, etc), marital status (married, unmarried). When you are working with a categorical variable, do store it as a factor! R will then recognise it being categorical, and handle it as such when doing calculations or making plots. Oefenopdracht (thuis) Maak de onderstaande opdrachten m.b.v. R. Noteer de gebruikte R code in een R script. Zorg ervoor dat je de code voorziet van uitleg in de vorm van comments (regels die beginnen met #) en dat je ook de antwoorden op de vragen in de vorm van comments in het R script noteert. Komende les bespreken we deze opdracht. In deze opdracht ga je werken met de Glass dataset. Deze dataset is een onderdeel van de mlbench package. Dit package is al geïnstalleerd. Laad het package in R m.b.v. de library() functie. Open de Glass dataset in R via deze regel code: data(Glass). Nu je de dataset hebt geopend in R, kun je informatie over de dataset gaan verzamelen: Wat voor data type (vector, list, data.frame, tibble etc.) is de Glass dataset? Wat zijn de dimensies (aantal rijen/kolommen) van de Glass dataset? Gebruik de help functie om informatie te vinden over de Glass dataset. Beschrijf kort in eigen woorden wat voor data deze dataset bevat. We gaan nu twee kleinere datasets maken uit de Glass dataset: Maak een nieuwe dataset met daarin alleen de data voor glastype 1. Je kunt hiervoor de volgende code gebruiken: glass1 &lt;- Glass[Glass$Type == 1,]. Maak ook een nieuwe dataset met daarin alleen de data voor glastype 5. Pas hiervoor de code zoals genoemd bij vraag (a) aan. Hoeveel metingen (= aantal rijen) zijn er gedaan voor glastype 1? En hoeveel metingen voor glastype 5? We willen kijken wat de verschillen zijn tussen glastype 1 en glastype 5: Wat is het gemiddelde ijzergehalte voor glastype 1? En wat is het gemiddelde ijzergehalte voor glastype 5? Wat is het minimale siliciumgehalte van glastype 5? En wat is het maximale siliciumgehalte van glastype 5? Is er een verschil in brekingsindex tussen glastype 1 en 5? Beargumenteer je antwoord. Bereken hiervoor de gemiddelde waarden en de standaardafwijking (sd functie in R). Maak een tibble waarin je de resultaten van opgave 4c samenvat. De kolommen van de tibble zijn ‘glastype’, ‘brekingsindex_gemiddelde’, en ‘brekingsindex_stdev’. Overview R functions first two lessons To create an object function package c() base list() base data.frame() base tibble() tibble as_tibble() tibble factor() forcats To inspect type of datastructure function package is.vector() base is.data.frame() base is.list() base is_tibble() tibble typeof() base str() utils class() base parse_factor() readr Properties of object function package dim() base length() base nchar() base View contents of object function package View() utils names() base head() utils tail() utils Arithmetic function package sum() base mean() base median() stats summary() base na.omit() stats min() base max() base runif() stats seq() base count() tidyverse Data transformation function package sort() base paste() base paste0() base append() base "],["lesson-3---importing-data-tidy-data.html", "Lesson 3 - Importing data &amp; tidy data Learning objectives Importing text files Setting variable (column) names Importing Excel files Saving and importing R_Objects Inspecting your data What is tidy data? Making your data tidy: gathering Making_your_data_tidy: spreading Making_your_data_tidy: separate Checking normality map functions Histograms Overview R functions", " Lesson 3 - Importing data &amp; tidy data Learning objectives After this lesson: You can import data into the R environment You know how to inspect your data You can make your data “tidy” Importing text files So far, we have worked with built-in datasets. In real life you will generate data yourself or you will obtain data from other researchers or public sources such as NCBI. To start working with your own datasets, data needs to be read by R and stored as R objects. We can use a variety of read_function() from the the tidyverse packages to import plain text datafiles as tibbles (sort of like a data table). These functions work the same. If you have mastered one you can use the other functions in a similar way: read_csv() ## comma separated values read_csv2() ## semicolon separated values read_tsv() ## tab separated values read_delim() ## custom defined delimiter, for example: read_delim(&quot;voorbeeld.csv&quot;, delim=&quot;&amp;&quot;) read_fwf() ## fixed width fields read_table() ## white space separated separators Before choosing the right read_function() you need to know how the values of your dataset are separated. You can view files by clicking on them in the files tab (lower right corner of the screen). Navigate to the folder data/lesson3. try clicking on a few txt files and inspect what they look like. click on heights.csv, select view Values of the heights.csv file are separated by comma’s. Even though the filename ends on csv (comma separated values) always inspect the contents of a datafile before loading it into R! We can use the read_csv() function: Example library(tidyverse) heights &lt;- read_csv(&quot;data/lesson3/heights.csv&quot;) ## Rows: 1192 Columns: 6 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): sex, race ## dbl (4): earn, height, ed, age ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. you can view the data by typing the object name in the console (or in a script and run that line): heights R assumes the decimal separator is a “point”. If your datafile has a comma as a decimal separator (most european datafiles) you have to tell the read_function() this! We will use a datafile which is present in the server folder data/lesson3 named heights3.txt. Inspect the datafile heights3.txt. As you can see, the values are tab separated and the decimal separator is a comma. We will first use the read_tsv() function with default settings: Example heights_3 &lt;- read_tsv(&quot;data/lesson3/heights3.txt&quot;) ## Rows: 1192 Columns: 6 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;\\t&quot; ## chr (2): sex, race ## dbl (3): earn, ed, age ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. heights_3 ## # A tibble: 1,192 × 6 ## earn height sex ed age race ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 50000 7.44e14 male 16 45 white ## 2 60000 6.55e14 female 16 58 white ## 3 30000 6.36e14 female 16 29 white ## 4 50000 6.31e14 female 16 91 other ## 5 51000 6.34e14 female 17 39 white ## 6 9000 6.44e14 female 15 26 white ## 7 29000 6.17e14 female 12 49 white ## 8 32000 7.27e14 male 17 46 white ## 9 2000 7.20e14 male 15 21 hispanic ## 10 27000 7.22e14 male 12 26 white ## # … with 1,182 more rows The values of variable height are not shown correctly: row1 original value = 74,4244387818035. row1 converted value = 7.44e14 We have to instruct read_tsv() that the comma acts a decimal separator by using the locale argument in combination with the locale() function: Example heights_3 &lt;- read_tsv(&quot;data/lesson3/heights3.txt&quot;, locale = locale(decimal_mark = &quot;,&quot;)) ## Rows: 1192 Columns: 6 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;\\t&quot; ## chr (2): sex, race ## dbl (4): earn, height, ed, age ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. heights_3 ## # A tibble: 1,192 × 6 ## earn height sex ed age race ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 50000 74.4 male 16 45 white ## 2 60000 65.5 female 16 58 white ## 3 30000 63.6 female 16 29 white ## 4 50000 63.1 female 16 91 other ## 5 51000 63.4 female 17 39 white ## 6 9000 64.4 female 15 26 white ## 7 29000 61.7 female 12 49 white ## 8 32000 72.7 male 17 46 white ## 9 2000 72.0 male 15 21 hispanic ## 10 27000 72.2 male 12 26 white ## # … with 1,182 more rows Now the values of the height variable are read in correctly. extracting numbers The read_function() (in this case read_tsv) guesses the datatype of each column and tries to read and convert the data in the right format. A problem you might encounter are numbers surrounded by non-numerical characters such as % of currency characters. For example $1,23 or 31%. To extract only the number we can use two options: The function parse_number() The argument col_types in combination with the function cols(). To demonstrate number extraction out of a character value we will use file dollar.txt in the server folder data/lesson3 Example dollar &lt;- read_tsv(&quot;data/lesson3/dollar.txt&quot;) ## Rows: 5 Columns: 2 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;\\t&quot; ## chr (1): amount ## dbl (1): person ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. dollar # variable amount is of data type character (chr) ## # A tibble: 5 × 2 ## person amount ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 $100 ## 2 2 $200 ## 3 3 $300 ## 4 4 $400 ## 5 5 $500 parse_number(dollar$amount[1]) # parse_number() extracts numeric value out of $100 ## [1] 100 dollar$amount &lt;- parse_number(dollar$amount) # replace all character strings for numeric values of the variable &quot;amount&quot; dollar # check that numeric values are extracted and returned ## # A tibble: 5 × 2 ## person amount ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 100 ## 2 2 200 ## 3 3 300 ## 4 4 400 ## 5 5 500 The alternative solution is to use the read_function() and change the col_types argument. The col_types argument is used in combination with the col() and the col_number() function (of which the latter is equivalent to the parse_number() function). The col() function is used to select all variables and the col_number() function is used to select a specific variable (in this case “amount”) to extract the number out of a character vector. Example dollar_2 &lt;- read_tsv(&quot;data/lesson3/dollar.txt&quot;, col_types = cols(amount = col_number())) dollar_2 # numeric values are extracted and returned . Data type is double (dbl) ## # A tibble: 5 × 2 ## person amount ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 100 ## 2 2 200 ## 3 3 300 ## 4 4 400 ## 5 5 500 Exercise 3 Import data datafile heights4.txt in server folder data/lesson3 into R. Click for the answer heights4 &lt;- read_delim(&quot;data/lesson3/heights4.txt&quot;, delim=&quot;@&quot;, locale = locale(decimal_mark = &quot;,&quot;)) ## Rows: 1192 Columns: 6 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;@&quot; ## chr (2): sex, race ## dbl (4): earn, height, ed, age ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. heights4 ## # A tibble: 1,192 × 6 ## earn height sex ed age race ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 50000 74.4 male 16 45 white ## 2 60000 65.5 female 16 58 white ## 3 30000 63.6 female 16 29 white ## 4 50000 63.1 female 16 91 other ## 5 51000 63.4 female 17 39 white ## 6 9000 64.4 female 15 26 white ## 7 29000 61.7 female 12 49 white ## 8 32000 72.7 male 17 46 white ## 9 2000 72.0 male 15 21 hispanic ## 10 27000 72.2 male 12 26 white ## # … with 1,182 more rows Exercise 3 Import data datafile salary.csv in server folder data/lesson3 into R. Convert character vectors with numbers to numeric values only. The thousand separator is a dot ! Click for the answer salary &lt;- read_csv2(&quot;data/lesson3/salary.csv&quot;, col_types = cols(Salary_2018 = col_number(), Increase_in_salary_2018 = col_number())) ## ℹ Using &quot;&#39;,&#39;&quot; as decimal and &quot;&#39;.&#39;&quot; as grouping mark. Use `read_delim()` for more control. salary ## # A tibble: 10 × 4 ## Employee Group Salary_2018 Increase_in_salary_2018 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 2000 0.02 ## 2 2 1 2000 0.023 ## 3 3 1 2000 0.016 ## 4 4 1 2000 0.032 ## 5 5 1 2000 0.0194 ## 6 6 2 3000 0.032 ## 7 7 2 3000 0.016 ## 8 8 2 3000 0.0343 ## 9 9 2 3000 0.024 ## 10 10 2 3000 0.011 Setting variable (column) names By default read_function() uses the first line of a dataset as variable names (column names). There are two situations where we have to change the behaviour of the read_function() If a dataset doesn’t contain variable names (=column names) use the option col_names = FALSE. The first line of the dataset will be ignored as variable names. The columns will be labelled automatically from X1 to Xn. If you want to manually add variable names you can provide a vector with the variable names tot the col_names option: col_names = c(“variable_name1”, “variable_name2”, “variable_name3”) Download dataset heights1.csv and move the file to the server folder data/lesson3. To move files between the server and your computer: Go to the server, select the folder you want to upload something to. click on Upload In the upload menu, click Choose File and select heights.csv where you just downloaded it. inspect heights.csv This file lacks variable names: Example heights_1 &lt;- read_csv(&quot;data/lesson3/heights1.csv&quot;, col_names = FALSE) # first line will not be used as variable names head(heights_1) ## # A tibble: 6 × 6 ## X1 X2 X3 X4 X5 X6 ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 50000 74.4 male 16 45 white ## 2 60000 65.5 female 16 58 white ## 3 30000 63.6 female 16 29 white ## 4 50000 63.1 female 16 91 other ## 5 51000 63.4 female 17 39 white ## 6 9000 64.4 female 15 26 white heights_1_1 &lt;- read_csv(&quot;data/lesson3/heights1.csv&quot;, col_names = c(&quot;name1&quot;, &quot;name2&quot;, &quot;name3&quot;, &quot;name4&quot;, &quot;name5&quot;, &quot;name6&quot;)) # added custom variable names by providing a vector containing the names of the variables # in this case: &quot;name1, name2 name3 etc&quot; head(heights_1_1) ## # A tibble: 6 × 6 ## name1 name2 name3 name4 name5 name6 ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 50000 74.4 male 16 45 white ## 2 60000 65.5 female 16 58 white ## 3 30000 63.6 female 16 29 white ## 4 50000 63.1 female 16 91 other ## 5 51000 63.4 female 17 39 white ## 6 9000 64.4 female 15 26 white Variables names can always be modified by using: The function colnames() The function rename() Example heights_1 &lt;- read_csv(&quot;data/lesson3/heights1.csv&quot;, col_names = FALSE) colnames(heights_1)&lt;- c(&quot;name1&quot;, &quot;name2&quot;, &quot;name3&quot;, &quot;name4&quot;, &quot;name5&quot;, &quot;name6&quot;) # change all column names colnames(heights_1)[3] &lt;- &quot;new_name3&quot; # change the name of a specified column using an index heights_1_1 &lt;-rename(heights_1_1, new_name4 = name4) # change the name of a specified column using rename() (2) if variable names are present but not at the first line use te options skip=N or comment = “#” Download dataset heights2.txt and move the file to the server folder data/lesson3. This file contains variable names but not at the first row. Note that the values are separated by tabs! First we have to inspect the datafile: click on the file. The first two lines are comments starting with the # sign. The variable names are present at row3. To read in the file we will use read_tsv() Example heights_2&lt;-read_tsv(&quot;data/lesson3/heights2.txt&quot;, skip = 2) # first two rows are not imported in object heights_2 heights_2 ## # A tibble: 1,192 × 6 ## earn height sex ed age race ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 50000 74.4 male 16 45 white ## 2 60000 65.5 female 16 58 white ## 3 30000 63.6 female 16 29 white ## 4 50000 63.1 female 16 91 other ## 5 51000 63.4 female 17 39 white ## 6 9000 64.4 female 15 26 white ## 7 29000 61.7 female 12 49 white ## 8 32000 72.7 male 17 46 white ## 9 2000 72.0 male 15 21 hispanic ## 10 27000 72.2 male 12 26 white ## # … with 1,182 more rows Alternatively we can use the comment = “#” option Example heights_2&lt;-read_tsv(&quot;data/lesson3/heights2.txt&quot;, comment = &quot;#&quot;) ## lines starting with a # sign are ignored for read in heights_2 ## # A tibble: 1,192 × 6 ## earn height sex ed age race ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 50000 74.4 male 16 45 white ## 2 60000 65.5 female 16 58 white ## 3 30000 63.6 female 16 29 white ## 4 50000 63.1 female 16 91 other ## 5 51000 63.4 female 17 39 white ## 6 9000 64.4 female 15 26 white ## 7 29000 61.7 female 12 49 white ## 8 32000 72.7 male 17 46 white ## 9 2000 72.0 male 15 21 hispanic ## 10 27000 72.2 male 12 26 white ## # … with 1,182 more rows Exercise 3 Import datafile iris in server folder data/lesson3 into R. This file lacks column names and starts with comments. Import the datafile without the comment lines and add columns names “Sepal.Length”, “Sepal.Width”, “Petal.Length”, “Petal.Width”, “Species” to the imported dataset. Click for the answer iris_flower&lt;-read_tsv(&quot;data/lesson3/iris&quot;, comment = &quot;#&quot;, col_names = c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;, &quot;Species&quot;)) ## Rows: 100 Columns: 5 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;\\t&quot; ## chr (1): Species ## dbl (4): Sepal.Length, Sepal.Width, Petal.Length, Petal.Width ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Importing Excel files To import Excel files directly into R we can use read_excel() function of the readxl package which is part of tidyverse. When Using read_excel() Excel datasheets will be converted to a tibble. read_excel() reads both xls and xlsx files and detects the format from the extension. More information about the readxl package can be found at tidyverse.org NOTE: Sometimes it’s more convenient to organise your data in Excel and copy the values as plain text in a text editor such as notepad. Subsequently, import the file using the appropiate read_function() described on the previous page. Example library(tidyverse) library(readxl) # you have to explicity load the readxl library excel_sheets(&quot;data/lesson3/datasets.xlsx&quot;) # inspect how many datasheets are present in the excel file ## [1] &quot;iris&quot; &quot;mtcars&quot; &quot;chickwts&quot; &quot;quakes&quot; &quot;iris_2&quot; &quot;quakes_1&quot; datasets_iris&lt;-read_excel(&quot;data/lesson3/datasets.xlsx&quot;) # default behaviour is to import the first datasheet. Excel files can have multiple datasheets. To specifically select a datasheet use the sheet = “sheetname” or sheet = number option Example datasets_mtcars&lt;-read_excel(&quot;data/lesson3/datasets.xlsx&quot;, sheet = &quot;mtcars&quot;) # select Excel datasheet named &quot;mtcars&quot; datasets_quakes&lt;-read_excel(&quot;data/lesson3/datasets.xlsx&quot;, sheet = 4) # select the 4th Excel datasheet (= &quot;quakes&quot;) Datafiles in Excel might not always contain column names (=variable names). If the first row doesn’t contain column names we have to tell read_excel() to treat the first row as values and not as variable names by using the col_names=FALSE option. Example datasets_iris_2&lt;-read_excel(&quot;data/lesson3/datasets.xlsx&quot;, sheet = 5, col_names = FALSE) ## New names: ## • `` -&gt; `...1` ## • `` -&gt; `...2` ## • `` -&gt; `...3` ## • `` -&gt; `...4` ## • `` -&gt; `...5` If you want to rename the column names you can use the rename() or the colnames() functions (see Setting variable (column) names) Exercise 3 Import sheet “quakes_1” from datafile datasets.xlsx in server folder data/lesson3 into R. Column names are not listed in datasets.xlsx. Import the datafile and add columns names “lat” “long” “depth” “mag” “stations” to the imported data. Click for the answer library(readxl) read_excel(&quot;data/lesson3/datasets.xlsx&quot;, sheet = &quot;quakes_1&quot;, col_names = c(&quot;lat&quot;, &quot;long&quot;, &quot;depth&quot;, &quot;mag&quot;, &quot;stations&quot;)) ## # A tibble: 1,000 × 5 ## lat long depth mag stations ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -20.4 182. 562 4.8 41 ## 2 -20.6 181. 650 4.2 15 ## 3 -26 184. 42 5.4 43 ## 4 -18.0 182. 626 4.1 19 ## 5 -20.4 182. 649 4 11 ## 6 -19.7 184. 195 4 12 ## 7 -11.7 166. 82 4.8 43 ## 8 -28.1 182. 194 4.4 15 ## 9 -28.7 182. 211 4.7 35 ## 10 -17.5 180. 622 4.3 19 ## # … with 990 more rows Saving and importing R_Objects To import data in plain text format we use the function read_function() To import data in Excel format we use the function read_excel() It is also possible to save an R object directly to the server and to read the file from the server in the R environment: To save an R object to the server use the function saveRDS() To import data that is stored as an R object we make use of the function readRDS() Example library(dslabs) # we will use a dataset called olive from the dslabs package View(olive) saveRDS(olive, &quot;data/lesson3/olive.rds&quot;) # object olive is saved as file olive.rds in server folder data/lesson3 To import this R object from the server into RStudio use readRDS(). This function takes 1 argument. Assign the R object from the server to a new object name in Rstudio! Example olive_from_server &lt;- readRDS(&quot;data/lesson3/olive.rds&quot;) ## the path to the file must be quoted Inspecting your data Data wrangling: the process of transforming and mapping data from a “raw” data format into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics and visualization ([wikipedia] (https://en.wikipedia.org/wiki/Data_wrangling)) The first step in data wrangling is to inspect the structure of the dataset. We’ve already seen some functions to inspect the data. Important questions are: What is the datastructure? How many variables does the dataset contain? What are the names of the variables? What are the data types of the variables? What are possible categories (factors)? What are the levels of the categories? Are there NA values? Is the data tidy? Let’s recall some previously used functions to inspect the data. We will use data from the “dslabs” package. Example library(tidyverse) library(dslabs) ?divorce_margarine # information about the divorce_margarine dataset from the &quot;dslabs&quot; package divorce_margarine_tbl &lt;- as_tibble(divorce_margarine) # converting dataframe to tibble divorce_margarine_tbl ## # A tibble: 10 × 3 ## divorce_rate_maine margarine_consumption_per_capita year ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 5 8.2 2000 ## 2 4.7 7 2001 ## 3 4.6 6.5 2002 ## 4 4.4 5.3 2003 ## 5 4.3 5.2 2004 ## 6 4.1 4 2005 ## 7 4.2 4.6 2006 ## 8 4.2 4.5 2007 ## 9 4.2 4.2 2008 ## 10 4.1 3.7 2009 Let’s inspect another dataset from the dslabs package: gapminder Example ?gapminder # information about the &quot;gapminder&quot; dataset gapminder_tbl &lt;- as_tibble(gapminder) # converting dataframe to tibble gapminder_tbl ## # A tibble: 10,545 × 9 ## country year infant_mortality life_expectancy fertility population gdp continent region ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 Albania 1960 115. 62.9 6.19 1636054 NA Europe South… ## 2 Algeria 1960 148. 47.5 7.65 11124892 1.38e10 Africa North… ## 3 Angola 1960 208 36.0 7.32 5270844 NA Africa Middl… ## 4 Antigua and Barb… 1960 NA 63.0 4.43 54681 NA Americas Carib… ## 5 Argentina 1960 59.9 65.4 3.11 20619075 1.08e11 Americas South… ## 6 Armenia 1960 NA 66.9 4.55 1867396 NA Asia Weste… ## 7 Aruba 1960 NA 65.7 4.82 54208 NA Americas Carib… ## 8 Australia 1960 20.3 70.9 3.45 10292328 9.67e10 Oceania Austr… ## 9 Austria 1960 37.3 68.8 2.7 7065525 5.24e10 Europe Weste… ## 10 Azerbaijan 1960 NA 61.3 5.57 3897889 NA Asia Weste… ## # … with 10,535 more rows levels(gapminder_tbl$continent) # categories in the continent variable ## [1] &quot;Africa&quot; &quot;Americas&quot; &quot;Asia&quot; &quot;Europe&quot; &quot;Oceania&quot; sum(is.na(gapminder_tbl$infant_mortality)) # count the missing (NA) values ## [1] 1453 The last question of the inspection list is: Is the data tidy? Before we can answer this question we have to know what tidy data is. What is tidy data? Tidy data is a way of organising your data in a neat and structured way. If you make your data tidy, it is ensured that it is compatible with data analysis tools from the tidyverse package. A detailed explanation can be found in chapter12 in the e-book R for Data Science However, it is important to note that data does not always need to be tidy. Other R packages require different data organisation formats! So what is tidy data: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. Table1: Example of tidy data country year population birth rate mali 2001 10.000.000 6.88 mali 2010 15.000.000 6.06 sweden 2001 9.000.000 1.57 sweden 2010 10.000.000 1.85 Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. So the following table would by untidy, as there are multiple observations per row: Table2: Example of untidy data student EDCC molbio immunologie BID Pietje 7.5 6 8.2 8 Marietje 8 7.9 5 9 This would be the tidy version: Table3: Example of untidy data made tidy student course grade Pietje EDCC 7.5 Pietje molbio 6 Pietje immunologie 8.2 Pietje BID 8 Marietje EDCC 8 Marietje molbio 7.9 Marietje immunologie 5 Marietje BID 9 Exercise 3 {-} The package tidyverse contains the built-in datasets: table1 table2 table3 table4a table4b Write code to inspect the data and argue if the tables are tidy Click for the answer To inspect for instance table1, just write table1 in the console window. There are three rules to make a dataset tidy: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. table1: tidy table2: non-tidy, violation of rule2. An observation is a country in a year, but each observation is spread across two rows. table3: non-tidy, violation of rule3. The “rate” column contains both cases and population variables table4a/b: non-tidy, violation of rule 1 and 2. The column names “1999” and “2000” represent values of the year variable, and each row represents two observations, not one. Making your data tidy: gathering In this exercise we noted that table4a is not tidy: the column names ‘1999’ and ‘2000’ represent values of the year variable, and each row represents two observations, not one. To merge columns ‘1999’ and ‘2000’ into 1 column we can use the tidyverse function pivot_longer(): table4a_tidy &lt;- pivot_longer(data = table4a, cols = c(&#39;1999&#39;, &#39;2000&#39;), names_to = &quot;year&quot;, values_to = &quot;cases&quot;) table4a_tidy ## # A tibble: 6 × 3 ## country year cases ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 ## 2 Afghanistan 2000 2666 ## 3 Brazil 1999 37737 ## 4 Brazil 2000 80488 ## 5 China 1999 212258 ## 6 China 2000 213766 Alternatively we can use the pipe to redirect data to the pivot_longer() function: Example table4a_tidy &lt;- table4a %&gt;% pivot_longer(cols = c(&#39;1999&#39;, &#39;2000&#39;), names_to = &quot;year&quot;, values_to = &quot;cases&quot;) Important: from now on we will often use the pipe %&gt;% to redirect data into an R function: The %&gt;% pipe symbol can be read as “followed by”. The %&gt;% symbol directs the OUTPUT of one R function as INPUT to the following R function. dataset %&gt;% function() %&gt;% function() .. and so on This way the steps of data manipulation are easier to follow for the reader. It is good practice to write clean / clear code that can be understand by other people. Exercise 3 Import datafile the first sheet gene_expression_c10.xlsx into R (it is already in the server folder data/lesson3) What are the dimension of the dataset? Click for the answer #(a) library(readxl) gene_expression &lt;- read_excel(&quot;data/lesson3/gene_expression_c10.xlsx&quot;) #(b) dim(gene_expression) # or look in the environment panel, upper right part of the screen ## [1] 24574 6 Exercise 3 Make the dataset of exercise_5.1 tidy What are now the dimension of the tidy dataset? Click for the answer #(a) gene_expression_tidy &lt;- pivot_longer(data = gene_expression, cols = c(&#39;0h&#39;, &#39;3h&#39;,&#39;12h&#39;, &#39;24h&#39;, &#39;48h&#39;), names_to = &quot;time(hrs)&quot;, values_to = &quot;expression&quot;) # or: gene_expression_tidy &lt;- gene_expression %&gt;% pivot_longer(cols = c(&#39;0h&#39;, &#39;3h&#39;,&#39;12h&#39;, &#39;24h&#39;, &#39;48h&#39;), names_to = &quot;time(hrs)&quot;, values_to = &quot;expression&quot;) #(b) dim(gene_expression_tidy) ## [1] 122870 3 Making_your_data_tidy: spreading In the this exercise we noted that table2 is not tidy: 2 row represents 1 observation. Note that variable column “Type” contains the names of the variables “Cases” and “Population”. This is the reason that two rows represent 1 observation. To merge the two rows, representing 1 observation, into 1 row we use the tidyverse function pivot_wider(). table2_tidy &lt;- pivot_wider(data = table2, names_from = &quot;type&quot;, values_from = &quot;count&quot;) table2_tidy ## # A tibble: 6 × 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 Alternatively we can use the pipe to redirect the data to the pivot_wider() function: Example table2_tidy &lt;- table2 %&gt;% pivot_wider(names_from = &quot;type&quot;, values_from = &quot;count&quot;) Exercise 3 Import the second data sheet from datafile gene_expression_c10.xlsx (from the previous question, in server folder data/lesson3) into R Make the dataset tidy Click for the answer #(a) gene_expression_sheet2 &lt;- read_excel(&quot;data/lesson3/gene_expression_c10.xlsx&quot;, sheet=2) #(b) gene_expression_sheet2_tidy &lt;- pivot_wider(data = gene_expression_sheet2, names_from = &quot;assay&quot;, values_from = &quot;value&quot;) # or: gene_expression_sheet2_tidy &lt;- gene_expression_sheet2 %&gt;% pivot_wider(names_from = &quot;assay&quot;, values_from =&quot;value&quot;) Making_your_data_tidy: separate In this exercise we noted that table3 is not tidy: 2 values are present in 1 cell. Example table3 ## # A tibble: 6 × 3 ## country year rate ## * &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745/19987071 ## 2 Afghanistan 2000 2666/20595360 ## 3 Brazil 1999 37737/172006362 ## 4 Brazil 2000 80488/174504898 ## 5 China 1999 212258/1272915272 ## 6 China 2000 213766/1280428583 To separate the values in two cells we use the tidyverse function separate(): table3_tidy &lt;- separate(table3, rate, into = c(&quot;cases&quot;, &quot;population&quot;)) table3_tidy ## # A tibble: 6 × 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 The original variable “rate” contained “character” values, namely the two combined values separated by a forward slash. After the separation each new column (“cases” and “population”) contain numeric values but the value types are still denoted as characters. To convert the character values into numeric values we use the convert option. Example table3_tidy &lt;- separate(table3, rate, into = c(&quot;cases&quot;, &quot;population&quot;), convert = TRUE) Normally you have to specify where the values need to be separated. In this case the forward slash character (“/”). The default option of the separate() funtion is to separate values based on any non-alphanumeric character (i.e. a character that isn’t a number or letter). It is good practice though to write the separation character explicity in your code by using the sep=“character” option. Example table3_tidy &lt;- separate(table3, rate, into = c(&quot;cases&quot;, &quot;population&quot;), convert = TRUE, sep = &quot;/&quot;) By using the sep= option we can also split values based on the a specific number of characters. For example we want to use the first three character of each country in our column instead of the full names. We can split the country names by the first three characters. Example # only 1 variable is defined: first 3 characters are stored in variable &quot;country&quot; replacing the original variable &quot;country&quot; separate(table3_tidy, country, into = c(&quot;country&quot;), sep = 3) ## # A tibble: 6 × 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afg 1999 745 19987071 ## 2 Afg 2000 2666 20595360 ## 3 Bra 1999 37737 172006362 ## 4 Bra 2000 80488 174504898 ## 5 Chi 1999 212258 1272915272 ## 6 Chi 2000 213766 1280428583 Exercise 3 Import datafile gene_expression_c10 (the one without the .xlsx) in the server folder data/lesson3 into R. The data colums are separated by a semicolon. Make the dataset tidy Click for the answer #(a) gene_expression_1 &lt;- read_csv2(&quot;data/lesson3/gene_expression_c10&quot;) ## ℹ Using &quot;&#39;,&#39;&quot; as decimal and &quot;&#39;.&#39;&quot; as grouping mark. Use `read_delim()` for more control. ## Rows: 24574 Columns: 2 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;;&quot; ## chr (2): Gene, 0h-3h-12h-24h-48h ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #(b) gene_expression_1_tidy &lt;- separate(gene_expression_1, &quot;0h-3h-12h-24h-48h&quot;, into = c(&quot;0h&quot;, &quot;3h&quot;, &quot;12h&quot;, &quot;24h&quot;, &quot;48h&quot;), sep = &quot;!&quot;, convert = TRUE) # or: gene_expression_1_tidy &lt;- gene_expression_1 %&gt;% separate(&quot;0h-3h-12h-24h-48h&quot;, into = c(&quot;0h&quot;, &quot;3h&quot;, &quot;12h&quot;, &quot;24h&quot;, &quot;48h&quot;), sep = &quot;!&quot;, convert = TRUE) First we separate the values in 5 new variables. But the data is not tidy yet. We have to apply the function pivot_longer() because the new variables are not true variables but values of the variable “time_after_induction” gene_expression_1_tidy2 &lt;- gene_expression_1_tidy %&gt;% pivot_longer( cols = c(&#39;0h&#39;, &#39;3h&#39;, &#39;12h&#39;, &#39;24h&#39;, &#39;48h&#39;), names_to = &quot;time_after_induction&quot;, values_to = &quot;gene_expression&quot;) Checking normality An important part of data analysis is to perform a statistical test to demonstrate that differences between experimental conditions are significant. In this course we will show how to perform a range of statistical test in R. These tests have been explained in detail in the 2nd year course TLSC-BID3V-17_Dataverwerking (see the figure below) and will only be briefly discussed again during this course. You previously executed them in SPSS or Excel, now we will show how to do them in R. If you realise you have quite forgotten what you learned in TLSC-BID3V-17_Dataverwerking and/or TLSC-EV1V-17, revisit the course content and refresh your knowledge! BID: canvas should be still up EV1V: click here for online reader Figure 6: Decision scheme for statistical testing Non-parametric tests in R actually just use the exact same syntax (way of writing the code) as the parametric equivalent, but using a different function name. Therefore, we will only cover the parametric tests within this course, en provide you with a table of non-parametric equivalent function names. The most common parametric statistical tests rely on the assumption that the residuals are normally distributed. (Not a clue what we’re talking about? Check the 1th year course on statistics here and also here). In the cases you will encounter here, this just means the same as the data within each group or variable coming from a population with a normal distribution. You will have learned in the 2nd year course on dataverwerking to perform a Shapiro-Wilk test to check this assumption when dealing with interval / ratio data. If any variable or any group data column is not normally distributed, Shapiro-Wilk test will give you a p-value below 0.05 and you have to make use of a non-parametric test. The Shapiro-Wilk test is used to determine whether your data are is normally distributed. First, let’s perform a Shapiro-Wilk in R:. To analyze the normality of a dataset is we will make use of the Shapiro-Wilk test (see figure 2). A statistical test serves to either accepts or reject the null-hypothesis. In the case of the Shapiro-Wilk test the null-hypothesis (H0) is defined as: H0: the sample came from a population that is not different from a normally distributed population. H1: the sample came from a population that is different from a normally distributed population. This means that if the p-value is &gt; 0,05 the H0 is accepted and we are allowed to perform a parametric test. (So in contrast to most statistical tests, we generally “want” a p&gt;0.05 here…) It is important to note that all variables being compared must be normally distributed in order to perform a parametric test. So if you want to investigate the correlation between the length of ears and the maximum running speed in rabbits, both your columns with data on ear length and maximum running speeds need to be “passing the Shapiro Wilk test” if you want to do parametric tests. And if you are interested in investigating whether mice who inhaled cigarette smoke for quite a while differ in stamina from a control group of mice, you will perform the Shapiro Wilk test on the data from both groups. Remember null hypothesis testing from previous statistics courses: The H0 (null hypothesis) generally states that there is no effect or difference. The p-value gives you the probability of seeing the data when the H0 would be true. If the p-value is below a value you agreed on before doing the testing (\\(alpha\\), generally 0.05) then you reject the H0. So, for example in a two sample design: if the data was very, very unlikely (low p-value) in the situation that there is no actual difference between groups (H0), we should reject the idea that there is no actual difference between groups (reject H0) and accept that there is probably a difference between groups (alternative hypothesis, H1). so: if p&lt;0.05, there is a significant effect/difference/etc. so: if p&gt;0.05, we cannot say there is a significant effect/difference/etc. To perform the Shapiro-Wilk test of normality for one variable at a time, use the function shapiro.test(). The function shapiro.test() takes a numeric vector as input and performs the shapiro-wilk test. The output of shapiro.test() is a list with fixed column names: [1] “statistic” “p.value” “method” “data.name” Let’s demonstrate the shapiro-wilk test with a dataset named smoking present in server folder “data/lesson3”. The smoking dataset is an unpaired experiment containing two different mice groups: control mice (“Controle”) and a group of mice continuously exposed to cigarette smoke (“Rook”). The measurement values represent minutes of swimming until exhaustion. Example library(tidyverse) smoking &lt;- read_tsv(&quot;data/lesson3/smoking&quot;) smoking #remember there are more rows, you only get shown the first few ## # A tibble: 21 × 2 ## Controle Rook ## &lt;dbl&gt; &lt;dbl&gt; ## 1 181 158 ## 2 193 166 ## 3 177 184 ## 4 183 164 ## 5 174 194 ## 6 169 163 ## 7 173 185 ## 8 197 165 ## 9 179 189 ## 10 197 170 ## # … with 11 more rows To perform a Shapiro-Wilk test, simply type: shapiro.test(smoking$Controle) ## ## Shapiro-Wilk normality test ## ## data: smoking$Controle ## W = 0.96615, p-value = 0.6473 shapiro.test(smoking$Rook) ## ## Shapiro-Wilk normality test ## ## data: smoking$Rook ## W = 0.94119, p-value = 0.2301 Both variables of the dataset “smoking” have a p-value &gt; 0,05 meaning we can perform a parametric test. map functions In the example we performed the Shapiro-Wilk test twice. This is because the shapiro.test() function accepts one vector at a time. This makes the code repetitive and generally in coding we want to avoid repetitive code as it is prone to errors. If we want to perform the same function on all variables of a dataframe (tibble) we can make use of a series of map functions: map() makes a list. map_lgl() makes a logical vector map_int() makes an integer vector map_dbl() makes a numerical vector map_chr() makes a character vector Exercise 3 Make sure you understand the difference between logical, integer, numerical, and character. Google it. For example, if we want the sum of each variable in dataset smoking we will use map_dbl(). The output of the sum() function will be stored in a numeric vector of type dbl (double) Example smoking %&gt;% map_dbl(sum) ## Controle Rook ## 3799 3698 If we want to perform the Shapiro-Wilk test on each variable of dataset smoking we will use map() as the output of the shapiro.test() function is a list. For each variable of the smoking dataset, a list is produced and these lists are combined and stored in a new list (thus, the elements of the final list consist of lists!) Example # shapiro.test() outputs multipe lines: use map() to store as list SW_smoking &lt;- smoking %&gt;% map(shapiro.test) class(SW_smoking) ## [1] &quot;list&quot; # the output of map() is a list containing two list elements # each list element contains the p-value of the Shapiro-Wilk test str(SW_smoking) ## List of 2 ## $ Controle:List of 4 ## ..$ statistic: Named num 0.966 ## .. ..- attr(*, &quot;names&quot;)= chr &quot;W&quot; ## ..$ p.value : num 0.647 ## ..$ method : chr &quot;Shapiro-Wilk normality test&quot; ## ..$ data.name: chr &quot;.x[[i]]&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;htest&quot; ## $ Rook :List of 4 ## ..$ statistic: Named num 0.941 ## .. ..- attr(*, &quot;names&quot;)= chr &quot;W&quot; ## ..$ p.value : num 0.23 ## ..$ method : chr &quot;Shapiro-Wilk normality test&quot; ## ..$ data.name: chr &quot;.x[[i]]&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;htest&quot; We see that the output of the Shapiro-Wilk test is stored as a list containing two elements, one for each variable. The elements in the list are lists themselves containing the summary information of the Shapiro-Wilk test. What we are really interested in is the p-value (NOTE: in the list stored as p.value). We can automatically substract the p-values by adding another map() function. As the p-values are numbers we select the map_dbl() function so the p-values will be stored in a numeric vector. Example smoking %&gt;% map(shapiro.test) %&gt;% map_dbl(&quot;p.value&quot;) ## Controle Rook ## 0.6473373 0.2300624 We see that both p-values are above 0,05. We accept the H0 and conclude that both samples are normally distributed. We are allowed to perform a parametric test. Exercise 7 Dataset Shapiro_Wilk present in server folder /data/lesson3 contains 10 samples with hematocrit values. Write an R code to determine which samples are not normally distributed. Click for the answer library(tidyverse) # load the data SW &lt;- read_tsv(&quot;data/lesson3/Shapiro_Wilk&quot;) ## Rows: 20 Columns: 10 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;\\t&quot; ## dbl (10): dataset1, dataset2, dataset3, dataset4, dataset5, dataset6, dataset7, dataset8, dataset9, da... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. SW_pvalues &lt;- SW %&gt;% map(shapiro.test) %&gt;% # perform Shapiro-Wilk test map_dbl(&quot;p.value&quot;) %&gt;% # extract p-values round(digits = 3) #round them down to 3 digits # select those that are significant (ie: not normally distributed) SW_pvalues[SW_pvalues &lt; 0.05] ## dataset1 dataset8 ## 0.031 0.019 Histograms A histogram is a representation of the distribution of numerical (continuous) data. A histogram divides the x-axis into equally spaced bins and then uses the height of a bar to display the number of observations that fall in each bin. Histograms are often used to inspect the distribution of a dataset. A fundamental skill in data science is to visualize your data clearly and honestly. We will use the package ggplot2 to make graphs in R (part of the tidyverse , so doesn’t have to be loaded separately from tidyverse). library(tidyverse) ggplot graphics are build with layers. In different layers, you provide ggplot with information about the data (data), how you want the graph to look like (aesthetics), and what kind of graph it should be (geom function). We will take a look at a hypothetical follow up experiment on the smoking data we used previously, using 200 mice who got nicotine patches or not. The dependent variable was minutes of swimming until exhaustion. I will spare the poor mice the exhaustion and generate the data for this example. Run this code to generate the fake data: set.seed(123) smoking2 &lt;- tibble( controle = rnorm(200,180,15), patch = rnorm(200,160,25) ) Let’s look at the minimum and maximum swimming times in the patch group: min(smoking2$patch) ## minimum minutes of swimming until exhaustion ## [1] 98.35255 max(smoking2$patch) ## maximum minutes of swimming until exhaustion ## [1] 224.2865 In order to see the distribution of the swimming times between the minimum and maximum values we will make a distribution plot, a histogram. It is build as: a layer with the data and aesthetics (aes) a layer telling ggplot we want a histogram a layer setting the title and axes ggplot(data = smoking2, aes(x = patch)) + geom_histogram()+ labs( title = &quot;Distribution of swimming times of patch mice&quot;, x= &quot;swimming time (min)&quot;, y= &quot;count&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We can choose the bin size. The bin size, is the range on the x-axis that represents the width of each bar in the histogram. We can change it if we like: ggplot(data = smoking2, aes(x = patch)) + geom_histogram(binwidth = 10)+ labs( title = &quot;Distribution of swimming times of control mice&quot;, x= &quot;swimming time (min)&quot;, y= &quot;count&quot;) To make separate histograms for the control and the patch group in the same figure, we could make the data tidy and add an extra layer to ggplot called facet_wrap(): # tidy data: each variable in 1 column smoking2_tidy &lt;- pivot_longer(data = smoking2, cols = c(controle, patch), names_to = &quot;group&quot;, values_to = &quot;swimming_time&quot;) # create histograms ggplot(data = smoking2_tidy, aes(x = swimming_time)) + geom_histogram(binwidth = 10)+ labs( title = &quot;Distribution of swimming times of control mice&quot;, x= &quot;swimming time (min)&quot;, y= &quot;count&quot;)+ facet_wrap(~ group) This is your first time using ggplot! Nice. Exercise 3 To generate random numbers of the standard normal distribution we make use of the function rnorm(). To generate 1000 random values of the standard normal distribution run rnorm(n = 1000). Use the function rnorm() to generate a tibble with 1000 random values of the standard normal distribution. Create a histogram of the 1000 random values of (a). Use a binwidth of 0.2. Click for the answer normal_dist &lt;- tibble(x = rnorm(1000)) Note that as we randomly generated numbers, your graph will look a bit different. You almost certainly generated different random nnumers than we did when we made this reader. ggplot(data = normal_dist, aes(x = x)) + geom_histogram(binwidth = 0.2) Overview R functions Data import of plain text files function package read_csv() tidyverse read_csv2() tidyverse read_tsv() tidyverse read_delim() tidyverse read_fwf() tidyverse read_table() tidyverse colnames() tidyverse rename() tidyverse saveRDS() base readRDS base Data import of Excel files function package read_excel() readxl excel_sheets() readxl Inspecting your data function package system() base as_tibble() tidyverse is.na() base levels() base Tidy Data function package pivot_longer() tidyverse pivot_wider() tidyverse separate() tidyverse "],["lesson-4---answering-research-questions.html", "Lesson 4 - Answering research questions Learning objectives Difference Questions Descriptive Questions Outliers Relation Questions Checklists Overview R functions", " Lesson 4 - Answering research questions Learning objectives After this lesson: You will have a basic understanding of different graph types and when to use them. You can visualize categorical data with ggplot. You can visualize continuous data with ggplot. You can summarize data using boxplots. you can perform the most common statistical tests with R. Note: this lesson assumes that youo have followed previous courses on statistics. We know you did, as we were teaching them! If you forgot all about them, please revisit BID and/or EV1V (click for online reader) note: if your data is not in a folder /data/, just ignore that part of the path name! Difference Questions When your research question is about a possible difference between samples (e.g. groups, conditions, etc) you often make a bar chart, and perform a t-test or ANOVA (if your data is normally distributed, see previous lesson). Practice making bar charts To start this lesson, we will first practice a bit with using ggplot2. We used it before to make histograms, but ggplot can make a lot of different graphs. A bar chart is a graph that presents categorical (nominal) data with rectangular bars with heights or lengths proportional to the values that they represent. The bars can be plotted vertically or horizontally. To create a bar chart we can either use: geom_bar(): by default, geom_bar() accepts one variable (categorical) as x-value. The number of occurrences for each category is calculated by default. geom_col(): by default, geom_col() accepts one variable (categorical) as x-value and one variable (values of each category such as occurences, mean, median, proportion, min, max and so on) as y-value. Note: arguments in the ggplot() function will be applied to all layers whereas arguments in the geom_function() layer will only be applied to that specific layer. Examples Let’s plot the amount of penguins per species in the palmerpenguins dataset (install it if needed): # Load the tidyverse package (only once for each R session) library(tidyverse) library(palmerpenguins) #let&#39;s use some penguin data. Penguins are cool. # Create a bar graph showing the counts based on variable species penguins %&gt;% # penguins is a datagframe within the palmerpenguins package ggplot() + geom_bar(aes(x = species)) + theme_minimal() + labs(title = &#39;Penguins per Species in the Palmerpenguins data&#39;) If we want to change the color of the bars based on the variable ‘species’ (3 different colours, because there are 3 different species), we can add an extra aestheti(c) ‘fill’. If we want to generally change the color of the bars, we can add ‘fill’ outside of the aesthetic. Examples ## &quot;fill&quot; is placed inside aesthetic penguins %&gt;% ggplot() + geom_bar(aes(x = species, fill = species)) + theme_minimal() + labs(title = &#39;Penguins per Species in the Palmerpenguins data&#39;) ## &quot;fill&quot; is placed outside the aesthetic penguins %&gt;% ggplot() + geom_bar(aes(x = species), fill = &quot;magenta&quot;) + theme_minimal() + labs(title = &#39;Penguins per Species in the Palmerpenguins data&#39;) ## &quot;color&quot; defines border color penguins %&gt;% ggplot() + geom_bar(aes(x = species), fill = &quot;magenta&quot;, colour=&quot;black&quot;) + theme_minimal() + labs(title = &#39;Penguins per Species in the Palmerpenguins data&#39;) Each geom_funtion has a standard way of calculating and presenting the data. This is defined by the ‘stat’ argument. The default for geom_bar() is ‘count’, meaning geom_bar() displays the number of occurences of each category in variable “class” on the y-axis. Alternatively, the geom_bar() can also calculate and display the proportion of each category (relative to the total number) on the y-axis. To do so we must explicitly instruct geom_bar() to change the default stat of count to proportion (..prop..). An alternative way to create a graph showing the proportion of each category is to first calculate the proportion values by using some handy functions of the tidyverse packages (see also lesson 5). The new values are then stored in a separate variable. Examples # Create a bar graph plotting the proportions (using ..prop..) # NB: &#39;group = 1&#39; indicates that the proportions are based on entire population ggplot(data = penguins) + geom_bar(aes(x = species, y =..prop.., group = 1)) + labs(title = &#39;proportions of Penguins per Species&#39;) # Calculate the counts of each category of the class variable using count() and # add a new variable that is a function of existing columns using mutate(). Use # the new variable to create a bar graph. # NB: count() creates a new variable &#39;n&#39; containing the counts penguins %&gt;% count(species) %&gt;% mutate(proportion = n / sum(n)) %&gt;% ggplot() + geom_col(aes(x = species, y = proportion, fill = species)) + labs(title = &#39;Also proportions of Penguins per Species&#39;) Exercise 4 Stars are classified by their spectra (the wavelengths that they absorb) and their temperature. There are seven main types of stars based on decreasing temperature. The categories are: O, B, A, F, G, K, and M. Use the dataset ‘stars’ from the ‘dslabs’ package to answer the following questions: Make a graph to show the number of stars for each type. Make a graph to show the number of stars for each type. Each bar has a different color. Add a black line to the different coloured bars of graph (b). Click if you want to see what the graphs should look like Click if you really tried and can’t replicate the graphs from the answer above library(dslabs) ggplot(data = stars, aes(x = type)) + geom_bar()+ labs(title = &quot;Number of stars for each type&quot;) ggplot(data = stars, aes(x = type)) + geom_bar(aes(fill = type))+ labs(title = &quot;Number of stars for each type&quot;) ggplot(data = stars, aes(x = type)) + geom_bar(aes(fill = type), colour = &quot;black&quot;)+ labs(title = &quot;Number of stars for each type&quot;) Grouped bar graphs In the examples above, the bar graphs consisted of single bars. In some cases you want to create grouped bar graphs. For example, given the penguin dataset introduced above, suppose that you want to know how many male and female penguins there are for each species. You can create a bar graph in a similar way as before, but now specifying ‘fill’ to be determined by ‘sex’: penguins %&gt;% count(species, sex) %&gt;% ggplot(aes(x = species, y = n, fill = sex)) + geom_col() We now created a grouped bar graph. However, the default of ggplot is to plot the different bars on top of each other and this is difficult to read. To plot the different bars next to each other, we can use the position_dodge() argument: penguins %&gt;% count(species, sex) %&gt;% ggplot(aes(x = species, y = n, fill = sex)) + geom_col(position = position_dodge()) Bar graphs with mean ± standard deviation Quite often, you will want to depict an average and standard deviation in a bar graph. For instance, the average flipper length in the different penguin species. To make a bar graph with the average flipper length, we will need to calculate it first. We will do a lot more data wrangling in the next lesson, so for now, we will just make a new tibble with the averages we need, grouped per species. There are some missing values for flipper length (I imagine not all penguins cooperated nicely when their feet were measured), and we remove those using na.rm=TRUE. Look at the content of flipper_summary by typing it in the console. flipper_summary &lt;- penguins %&gt;% group_by(species) %&gt;% summarize(mean_flipper=mean(flipper_length_mm, na.rm=TRUE), stdev=sd(flipper_length_mm, na.rm=TRUE)) And we can plot this using: flipper_summary %&gt;% ggplot(aes(x=species,y=mean_flipper, group=species, fill=species)) + geom_col(stat=&quot;identity&quot;)+ geom_errorbar(aes(ymin=mean_flipper-stdev, ymax=mean_flipper+stdev), width=.2)+ # the error bars labs(title = &quot;Figure X. average flipper length of penguins&quot;, subtitle = &quot;errorbars depict 1 standard deviation&quot;, x=&quot;penguin species&quot;, y=&quot;average flipper length (mm)&quot;) + theme(legend.position = &quot;none&quot;, text = element_text(size=16)) # legend is not needed ## Warning: Ignoring unknown parameters: stat Two sample t-tests Often, bar graphs are used to visualise possible differences between samples (e.g. between groups, conditions, etc). Generally, we then have a categorical variable on the x-axis (such as penguin species), and a continuous variable on the y-axis (such as flipper length in mm.). But as you have learned previously in statistics courses, you will have to do some statistics before we can confidently proclaim we see any differences between the conditions. (If you completely forgot what a t-test is, please refresh your memory here Before running a t-test in R we need to know: The experimental set-up (i.e. unpaired vs paired) Figure 7: Unpaired or paired experimental set-up examples whether the samples have equal variances (NOTE: only relevant with an unpaired experimental set up) Unpaired two sample t test As unpaired experiments are a bit more intuitive, we will start with an unpaired two sample t-test. 1. Get the data and prepare the data Here is some artificial data on ear length of rabbits: rabbit_ears &lt;- tibble( rabbitnr = seq(1,20), colour = c(rep(&quot;beige&quot;, 10),rep(&quot;brown&quot;, 10)), earlength = c(17.5, 13.0, 18.6, 19.4, 20.9, 18.9, 22.8, 20.3, 22.1, 20.4, 23.0, 24.1, 26.1, 25.2, 27.8, 28.5, 26.5, 21.5, 20.2, 18.8) ) colour_rabbits_summary &lt;- rabbit_ears %&gt;% group_by(colour) %&gt;% summarize(mean_ear=mean(earlength), stdev=sd(earlength)) 2. Plot the data ## Warning: Ignoring unknown parameters: stat Looking at the bar graph, the ear length brown rabbits seems to be a bit longer, on average, than in beige rabbits. Let’s investigate this. 3. Check normality Research question: Is there a difference in ear length between brown and beige rabbits? Exercise 7 As you have learned in lesson 3, we first need to check whether the data is normally distributed. Please check. HINT You need to use shapiro.test. We have two groups of rabbits, so both groups need to have normally distributed ear lengths. The ear length is currently in a tidy format, bu we previously mapped shapiro.test() to data in wide format… You have several options, all of which are fine. manually split the data using indexing (lesson 1) (specifically, look at the homework assignment after lesson 2). Realise that the colour variable is text data, not a number. use pivot_wider to make separate columns for ear length in the two groups (lesson 3) and then index the column you want (lesson 1) magically already know how to summarise (lesson 5) Click for the answer Let’s do option 1 for now. brown_ears &lt;- rabbit_ears[rabbit_ears$colour==&quot;brown&quot;,] beige_ears &lt;- rabbit_ears[rabbit_ears$colour==&quot;beige&quot;,] brown_ears$earlength %&gt;% shapiro.test() beige_ears$earlength %&gt;% shapiro.test() The shorter option 3 you have not learned yet rabbit_ears %&gt;% group_by(colour) %&gt;% summarise(p.value.sw = shapiro.test(earlength)$p.value) ## # A tibble: 2 × 2 ## colour p.value.sw ## &lt;chr&gt; &lt;dbl&gt; ## 1 beige 0.225 ## 2 brown 0.785 Both have a p-value &gt; 0,05 meaning we can perform a normal parametric t-test. 4. Check the variances Next, as we have an unpaired design, we need to know if the variance in ear length in the two groups of rabbits is very different. To test for equal variances we will make use of levene’s test. In the case of Levene’s test the null-hypothesis (H0) is defined as: H0: var1 = var2 (samples come from populations with variances that are not different.) H1: var1 ≠ var2 (samples come from populations with variances that are different from each other.) Levene’s test can be used to check for equal variances. To perform levene’s test use the function leveneTest() from the “car” package (“car” stands for “Companion to Applied Regression”, this package is not about cars.) Example # load the package and check out the function library(car) ?leveneTest() # check the arguments for running this function You can see that the input for levenetest() requires 1 vector with the data and another vector describing the groups. Our rabbit data is already in the correct format, but we have to convert the group variable (rabbit colour) to a factor to make sure R understands this is a categorical variable. Tip: If R is absolutely refusing to cooperate, make sure you defined your variables correctly as categorical (factor), numbers or text. Example library(car) leveneTest(rabbit_ears$earlength, as.factor(rabbit_ears$colour), center = mean) ## Levene&#39;s Test for Homogeneity of Variance (center = mean) ## Df F value Pr(&gt;F) ## group 1 0.8626 0.3653 ## 18 We find a p-value (Pr(&gt;F)) of 0.365, which is &gt;0.05, so H0 is accepted. The groups have equal variance. 5. Perform the t-test Finally we can perform a unpaired t-test with samples having equal variance. The t.test() function takes the following arguments: x : a (non-empty) numeric vector of data values. y : an optional (non-empty) numeric vector of data values. paired : a logical indicating whether you want a paired t-test.(determine by looking at the experimental design) var.equal : a logical variable indicating whether to treat the two variances as being equal (determine with a Levene’s test) conf.level : default 95% formula : optional formula defining the data and the groups written as data ~ groups (when your data is in tidy format) Before we perform the t.test we have again to define the H0 and H1: H0: μ1 = μ2 (brown and beige rabbits do not differ in mean ear length) H1: μ1 ≠ μ2 (brown and beige rabbits do differ in mean ear length) Example t.test(formula = rabbit_ears$earlength ~ rabbit_ears$colour, paired = FALSE, var.equal = TRUE) ## ## Two Sample t-test ## ## data: rabbit_ears$earlength by rabbit_ears$colour ## t = -3.5452, df = 18, p-value = 0.002312 ## alternative hypothesis: true difference in means between group beige and group brown is not equal to 0 ## 95 percent confidence interval: ## -7.612654 -1.947346 ## sample estimates: ## mean in group beige mean in group brown ## 19.39 24.17 Or if you just want the p-value, rounded down to 3 digits: t.test(formula = rabbit_ears$earlength ~ rabbit_ears$colour, paired = FALSE, var.equal = TRUE)$p.value %&gt;% round(.,3) ## [1] 0.002 5. Draw a conclusion Based on this p-value being below 0.05, we can conclude that there is a significant difference in ear length between brown rabbits (mean±sd = 24 ± 3 cm) and beige rabbits (19± 3 cm.), with brown rabbits having the on average longer ears. Note, the t.test() function has the option to enter the data separately. So, we don’t have to make the dataset tidy. It works on separate columns with data as well: brown_ears &lt;- rabbit_ears[rabbit_ears$colour==&quot;brown&quot;,] beige_ears &lt;- rabbit_ears[rabbit_ears$colour==&quot;beige&quot;,] # note that we don&#39;t use &quot;formula = &quot; now, and note the comma&#39;s t.test(brown_ears$earlength, beige_ears$earlength, paired = FALSE, var.equal = TRUE)$p.value %&gt;% round(.,3) Exercise 4 Here is some hypothetical data on the tail length of Siameze and Abyssinian cats. We are wondering if siamese cats have longer tails than Abyssinian cats. Run the following code to generate the data, and inspect the dataframe we get. # make up some data set.seed(672022) n&lt;-20 staartlengtes &lt;- rnorm(n, 30, 4) noise &lt;- rnorm(n, 0, 2.5) noise2 &lt;- rnorm(n, 0, 8.5) staartlengtes_S &lt;- staartlengtes + noise staartlengtes_A &lt;- staartlengtes*0.8 + noise2 cat_tails &lt;- tibble(breed = rep(c(&quot;Siamese&quot;,&quot;Abyssinian&quot;),each=n), tail_length = c(staartlengtes_S,staartlengtes_A)) Which of the two variables is categorical? Tell R that it is. Make a bar graph depicting this data. check the assumptions (normality and if relevant, equality of variances) perform the t-test. Note that we have a one-sided research question here, note the word “longer”. You can use the argument alternative = within t.test(). Check ?t.test formulate a conclusion. Click for answer A cat_tails$breed &lt;- as.factor(cat_tails$breed) Click for answer B plot_table &lt;- cat_tails %&gt;% group_by(breed) %&gt;% summarize(average = mean(tail_length, na.rm = TRUE), sd = sd(tail_length, na.rm = TRUE) ) # Plot data plot_table %&gt;% ggplot(aes(x = breed, y = average)) + geom_col(stat=&quot;identity&quot;, fill=&quot;steelblue&quot;)+ geom_errorbar( aes(x=breed, ymin=average-sd, ymax=average+sd), width=0.2)+ theme_classic() + # not necessary, I just like the white background labs(title = &quot;Tail length of Abyssinian and Siamese cats&quot;, subtitle = &quot;errorbars depict 1 standard deviation&quot;, x=&quot;cat breed&quot;, y=&quot;average tail length (cm)&quot;) ## Warning: Ignoring unknown parameters: stat Click for answer C # normality s_df &lt;- cat_tails[cat_tails$breed==&quot;Siamese&quot;,] a_df &lt;- cat_tails[cat_tails$breed==&quot;Abyssinian&quot;,] shapiro.test(s_df$tail_length)$p.value ## [1] 0.6034619 shapiro.test(a_df$tail_length)$p.value ## [1] 0.4663104 # equality of variances library(car) leveneTest(cat_tails$tail_length, cat_tails$breed, center = mean) ## Levene&#39;s Test for Homogeneity of Variance (center = mean) ## Df F value Pr(&gt;F) ## group 1 5.959 0.01941 * ## 38 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Both data columns are normally distributed, but the variances are not equal! Click for answer D note var.equal = FALSE because of the unequal variances note alternative = \"greater\" because of the one-sided question If you use the formula-statement in t.test, check the order of the factors with levels(cat_tails$breed)! (or type cat_tails$breed and look at the bottom at the factors. ) # siamese (first) &quot;greater than&quot; Abyssinian (second)? t.test(s_df$tail_length, a_df$tail_length, var.equal = FALSE, alternative = &quot;greater&quot;) #or: (less, because of the order of the factors in breed: Abyssinian is first) t.test(formula=cat_tails$tail_length~cat_tails$breed, var.equal = FALSE, alternative = &quot;less&quot;) Click for answer E As p&lt;0.05, we can conclude that Siamese cats (mean± sd = 34±3cm) have significant longer tails than Abyssinian cats (mean±sd=25±7cm). Paired two sample t test To investigate a difference question on paired data, we will make use of the dataset hematocrit_paired present in server folder data/lesson4. This dataset contains the hematocrit values (in %) of a group of athletes before and after altitude workout. Notice that each element (person) of the group is measured twice (before and after) and therefore this is a paired experimental setup. We want to know whether there is a difference in hematocrit values before vs. after the altitude workout. 1. Load the data We will load the data from a datafile on the server: #check which separator is used between the columns #We will use read_csv2() to import data file hematocrit_paired into R hematocrit_paired &lt;-read_csv2( &quot;data/lesson4/hematocrit_paired&quot; ) ## ℹ Using &quot;&#39;,&#39;&quot; as decimal and &quot;&#39;.&#39;&quot; as grouping mark. Use `read_delim()` for more control. ## Rows: 20 Columns: 2 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;;&quot; ## dbl (2): voor, na ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. hematocrit_tidy &lt;- pivot_longer(data = hematocrit_paired, cols = c(voor, na), names_to = &quot;condition&quot;, values_to = &quot;hematocrit&quot;) 2. Plot the data We can make a bar graph: hematocrit_tidy_summary &lt;- hematocrit_tidy %&gt;% group_by(condition) %&gt;% summarize(mean=mean(hematocrit), stdev=sd(hematocrit)) hematocrit_tidy_summary %&gt;% ggplot(aes(x=condition,y=mean)) + geom_col(stat=&quot;identity&quot;)+ geom_errorbar(aes(ymin=mean-stdev, ymax=mean+stdev), width=.2)+ # the error bars theme_classic()+ labs(title = &quot;Hematocrit before and after altitude workout &quot;, subtitle = &quot;errorbars depict 1 standard deviation&quot;, x=&quot;workout condition&quot;, y=&quot;average hematocrit (%)&quot;) + theme(legend.position = &quot;none&quot;, text = element_text(size=14)) # legend is not needed ## Warning: Ignoring unknown parameters: stat 3. Check normality First we have to check the assumption for doing a t-test: normally distributed residuals. However, which residuals should we be looking at here? In a paired experimental setup, data in the two conditions that are to be compared are not independent. Rather, they appear in pairs. In the hematocrit-dataset, data in each row (2 datapoints) belongs to the same participant. Some athletes in the experiment may just have a higher hematocrit than others, but this is not what we are interested in. We are interested in the difference between before and after the workout, regardless of individual baseline differences between athletes. If we would just do a normal unpaired t-test, effects of the workout (within subject effects) would be a lot harder to see because of all the individual differences (between subject effects) in which we are not interested. In a normal unpaired t-test, these sources of variance would be mixed. Luckily, we can get rid of the between subject variance by looking at the difference in hematocrit between the two measurements for each athlete and test if these differences are significantly different from 0. This is exactly what a paired-sample t-test does. Now, as a paired sample t-test is in fact a one-sample t test on the difference between conditions (before and after in this case), the normality assumption also considers these difference scores: hematocrit_difference &lt;- hematocrit_paired$na - hematocrit_paired$voor shapiro.test(hematocrit_difference)$p.value ## [1] 0.2921799 p-value &gt; 0,05. Accept H0: The differences are normally distributed. 4. Perform the t-test We can now perform a paired t-test. Before we can conduct the t-test we have to formulate the H0 and H1: H0: μ1 = μ2 (hematocrit values before and after workout do not differ) H1: μ1 ≠ μ2 (hematocrit values before and after workout do differ) Example t.test( hematocrit_paired$voor , hematocrit_paired$na , paired = TRUE )$p.value %&gt;% round( digits = 3 ) ## [1] 0.227 Compare this to the one sample t-test on whether the individual differences between before and after differ from 0. It will be the same: Example t.test( hematocrit_difference, mu=0)$p.value %&gt;% round( digits = 3 ) ## [1] 0.227 5. Draw a conclusion The p-value is &gt; 0,05: accepts H0. There is no statistically significant difference in hematocrit levels between the samples before and after altitude workout. Plotting significance We previously tested the difference in ear length between brown and beige rabbits, which was significant: p_to_plot &lt;- t.test(earlength ~ colour, data=rabbit_ears)$p.value %&gt;% round(3) Let’s put an asterix in the plot to communicate the the difference is in fact significant, using the ggsignif package Install the package first! You can also use annotate() to put text in a ggplot anywhere you like: library(ggsignif) colour_rabbits_summary %&gt;% ggplot(aes(x=colour,y=mean_ear, group=colour, fill=colour)) + geom_col(stat=&quot;identity&quot;)+ geom_errorbar(aes(ymin=mean_ear-stdev, ymax=mean_ear+stdev), width=.2)+ # the error bars labs(title = &quot;Figure X. average ear length of rabbits&quot;, subtitle = &quot;errorbars depict 1 standard deviation&quot;, x=&quot;rabbit colour&quot;, y=&quot;average ear length (cm)&quot;) + ylim(0, 35) + geom_signif( comparisons = list(c(&quot;beige&quot;, &quot;brown&quot;)), y_position = 30, tip_length = 0, vjust = 0.2, annotation = c(&quot;*&quot;) ) + annotate(&quot;text&quot;, x = 1.5, y = 35, label = &quot;two sample t-test p=0.002&quot;) ## Warning: Ignoring unknown parameters: stat ANOVA If we want to compare whether three (or more) samples are significantly different we make use of an ANOVA test. In one-way ANOVA test, a significant p-value indicates that some of the group means are different, but we don’t know which pairs of groups are different! ANOVA can be used to compare three or more groups/conditions. The R function to perform the ANOVA test is aov() in combination with the function summary.aov() 1. Load data, prepare data We will use the built-in dataset PlantGrowth. This dataset contains the results from an experiment to compare yields (as measured by dried weight of plants) obtained under a control and two different treatment conditions. # dataframe contains 2 variables: the group variable contains three groups head(PlantGrowth) ## weight group ## 1 4.17 ctrl ## 2 5.58 ctrl ## 3 5.18 ctrl ## 4 6.11 ctrl ## 5 4.50 ctrl ## 6 4.61 ctrl plantgrowth_graph &lt;- PlantGrowth %&gt;% group_by(group) %&gt;% summarise(mean=round(mean(weight),2), sd=round(sd(weight),2)) 2. Plot using bargraph plantgrowth_graph %&gt;% ggplot(aes(x=group, y=mean, group=group, fill=group)) + geom_col(stat=&quot;identity&quot;)+ geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2)+ # the error bars labs(title = &quot;Average dried plant weight &quot;, subtitle = &quot;control and two treatment conditions&quot;, x=&quot;treatment&quot;, y=&quot;yield (grams)&quot;) + theme(legend.position = &quot;none&quot;, text = element_text(size=16)) # legend is not needed ## Warning: Ignoring unknown parameters: stat 3. Check normality (Shapio-Wilk) of groups separately To conduct an ANOVA test data values in each group have to be normally distributed: The shapiro.wilk() function takes one group at a time. If we add the group variable then the shaprio-wilk test will be performed on the data of the three groups together. We first have to separate the three groups into three different variables. Example PlantGrowth_ctrl &lt;- PlantGrowth %&gt;% filter(group == &quot;ctrl&quot;) PlantGrowth_trt1 &lt;- PlantGrowth %&gt;% filter(group == &quot;trt1&quot;) PlantGrowth_trt2 &lt;- PlantGrowth %&gt;% filter(group == &quot;trt2&quot;) PlantGrowth_tibble &lt;- tibble(ctrl = PlantGrowth_ctrl$weight, trt1 = PlantGrowth_trt1$weight, trt2 = PlantGrowth_trt2$weight) PlantGrowth_tibble %&gt;% map(shapiro.test) %&gt;% map_dbl(&quot;p.value&quot;) %&gt;% round(digits = 3) ## ctrl trt1 trt2 ## 0.747 0.452 0.564 You can see that we have normally distributed data. 4. Perform ANOVA The aov() function minimally takes the following arguments: formula : A formula specifying the model. data : A data frame in which the variables specified in the formula will be found. The formula defines the group variables and the measurement variables to be test and is written as measurements ~ group. In our case the measurements are stored in the “weight” variable and the group information is stored in the “group” variable in the original dataset PlantGrowth. Before conducting the ANOVA test we have to define the H0 and the H1 : H0: μ1 = μ2 = μ3 (mean dried weight of plants in the 3 conditions is equal) H1: The means are not all equal (notice that this one is harder to write in mathematical symbols as this test won’t tell us which means are unequal!) Example plant_weight &lt;- aov(weight ~ group, PlantGrowth) summary.aov(plant_weight) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 2 3.766 1.8832 4.846 0.0159 * ## Residuals 27 10.492 0.3886 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Pr(&gt;F) is the p-value, you can see that it is &lt;0.05 5. Draw a conclusion P-value &lt; 0,05. So H0 is rejected and H1 is accepted: there is a significant difference between the groups in weight (although we don’t know exactly between which groups). 6. Post hoc tests If there is a significant effect of group here, we may want to check which groups differ. We do this by performing multiple two sample t-tests commparing each combination of two conditions. There is a convenience function to compare them all at once: pairwise.t.test(PlantGrowth$weight, PlantGrowth$group, p.adj = &quot;none&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: PlantGrowth$weight and PlantGrowth$group ## ## ctrl trt1 ## trt1 0.1944 - ## trt2 0.0877 0.0045 ## ## P value adjustment method: none The numbers are the p-values for the t-test with each combination of groups. However, as we perform multiple t-tests here, we need to lower the \\(alpha\\) (more on why this is needed in lesson 7.). We will lower it from the general 0.05, to 0.05 / amount_of_t_tests. So 0.05/3 = 0.017. Only p-values below 0.017 will now indicate a significant difference. The table shows that the difference between treatment 1 and treatment 2 is significant (p=0.0045) at an \\(/alpha\\) of 0.017. Exercise 4 Chemistry students determine the weight percentage of sodium in bags of potatos using an analytical instrument. Four groups of students divide each bag of chips into 4 equal part for a total of 12 bags. Each group measures the weight / weight % of sodium. The results are listed in the datafile potato present in server folder data/lesson4 Make a boxplot of the data Write an R code to perform all required statistical test to test the following H0 and H1: H0 : There is no difference in sodium weight/weight % between the 4 groups H1 : There is a difference in sodium weight/weight % between the 4 groups Click for the answer #(a) ## first data import potato &lt;- read_delim(&quot;data/lesson4/potato&quot;, delim = &quot;:&quot;, locale = locale(decimal_mark = &quot;,&quot;)) ## Rows: 11 Columns: 4 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;:&quot; ## chr (4): group1, group2, group3, group4 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## use map() to parse each column into a number. Convert output list to tibble potato &lt;- potato %&gt;% map(parse_number, locale = locale(decimal_mark = &quot;,&quot;)) %&gt;% as_tibble() potato_tidy &lt;- potato %&gt;% pivot_longer(cols = c(group1, group2, group3, group4), names_to = &quot;group&quot;, values_to = &quot;weight_%&quot;) potato_tidy %&gt;% ggplot(aes(x = group, y =`weight_%`, fill = group)) + geom_boxplot() + labs(title=&quot;sodium in bags of potatos measured by 4 groups of students&quot;, y= &quot;weight / weight % sodium&quot;)+ theme_classic() + #remove the gray background if you think it is ugly theme(legend.position = &quot;none&quot;) #legend is redundant, also increase text size #(b) ## shapiro-wilk test potato %&gt;% map(shapiro.test) %&gt;% map_dbl(&quot;p.value&quot;) %&gt;% round(digits = 3) ## group1 group2 group3 group4 ## 0.413 0.066 0.979 0.072 ## ANOVA: use the tidy object potato_tidy_aov &lt;- aov(`weight_%` ~ group, potato_tidy) summary.aov(potato_tidy_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 3 0.00027 0.000091 0.023 0.995 ## Residuals 40 0.15990 0.003998 ## if you only want to show the p-value of the ANOVA potato_tidy_aov_pvalue &lt;- summary.aov(potato_tidy_aov) potato_tidy_aov_pvalue[[1]]$`Pr(&gt;F)`[1] %&gt;% round(digit = 3) ## [1] 0.995 p&gt;0.05, so there is no significant difference in sodium weight/weight % between the 4 groups. One sample t-test A one sample design is actually just a research question regarding a difference, however instead of commparing two samples to each other, you compare one sample to a number / reference value. Suppose we test a new method for determining mercury and we test this method on a reference material that we know contains 29% mercury 1. Load data, prepare data We measured the material 30 times and obtained the following percentages: c(30.6, 30, 30.1, 31, 29.8, 29.1, 31.1, 29.5, 30.5, 29.6, 31.6, 30, 29.8, 30, 28.9, 29.3, 28.3, 29.2, 30.2, 30, 31.9, 29.4, 29.6, 30.2, 29.5, 29.5, 29.4, 29.2, 30, 30) ## [1] 30.6 30.0 30.1 31.0 29.8 29.1 31.1 29.5 30.5 29.6 31.6 30.0 29.8 30.0 28.9 29.3 28.3 29.2 30.2 30.0 ## [21] 31.9 29.4 29.6 30.2 29.5 29.5 29.4 29.2 30.0 30.0 m_data&lt;- tibble(mercury=mercury) Question: Is there a systematic error in our new method? So: is there a significant difference between our measurements and 29%? 2. Plot the data Exercise 4 Plot the data in a histogram. Click for the answer Looking at the graph, there could be a slight overestimation of the mercury content using our new method. ggplot(data = m_data, aes(x = mercury)) + geom_histogram(binwidth = 0.2) + theme_classic()+ labs(title = &quot;Distribution of mercury measurements&quot;, subtitle = &quot;using the new method&quot;, x=&quot;% mercury&quot;, y=&quot;count&quot;) 3. Check normality Exercise 4 Check if the mercury measurements are normally distributed Click for the answer shapiro.test(m_data$mercury) ## ## Shapiro-Wilk normality test ## ## data: m_data$mercury ## W = 0.94722, p-value = 0.1423 They are. We can perform normal parametric tests. 4. Perform one sample t-test To test if the measurements obtained with this new method significantly differ from the known mercury content, you can do a one sample t-test: t.test(m_data$mercury, mu = 29) ## ## One Sample t-test ## ## data: m_data$mercury ## t = 7.396, df = 29, p-value = 3.779e-08 ## alternative hypothesis: true mean is not equal to 29 ## 95 percent confidence interval: ## 29.87781 30.54886 ## sample estimates: ## mean of x ## 30.21333 It gives us a p-value far lower than 0.05. So we indeed find a systematic error in our measurements. 5. Draw a conclusion The average of our measurements is 30.2 and p&lt;0.05, so we have a significant overestimation of the mercury content with our new method. Detection You could of course use the same statistical test to see if there is for instance any mercury in a different material that should contain none at all. Then you just fill in zero for the mu. Let’s say we used the old, calibrated method and measured 5 times: 0.12, -0.23, 0.20, -0.14, 0.2. Let’s test this as well, even though it is not a lot of data: t.test(c(0.12, -0.23, 0.20, -0.14, 0.2), mu = 0) ## ## One Sample t-test ## ## data: c(0.12, -0.23, 0.2, -0.14, 0.2) ## t = 0.33292, df = 4, p-value = 0.7559 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -0.2201884 0.2801884 ## sample estimates: ## mean of x ## 0.03 Given these few measurements, we find no convincing evidence to say that there is any mercury in this material, as the p-value is larger than 0.05. We will however look at the possible trouble arising from these statements based on very low sample sizes in lesson 7. Descriptive Questions Some research questions do not necessarily rely on inductive statistics, but rather you are interested in for instance the spread or the distribution of a dataset. We covered descriptive statistics (mean, standard deviation etc) in the previous lesson, as well as histograms, but will focus a bit more on line graphs and boxplots here. Line graphs A line graph is a graph that connects data points with a line to visualise a (local) trend. The x-axis variable is continuous, preferably with similar intervals and most often involves time series but also concentrations, doses or any other increasing unit. For example, measurement of tumor size per week after treatment, number of species per increasing area unit or a calibration line with fixed increasing concentrations on the x-axis. Note: the x-values of a line graph should always be numeric!! (so no groups, factors etc. No qualitative variables, only quantitative.) Also, values inbetween the measured x-axis values should theoretically be possible. So the variable needs to be continuous. To create a line chart, we use the function geom_line(). We will use a new dataset called ‘gapminder’, which is part of the dslabs package. We will create a line graph showing the change in life expectancy from 1960-2016 in the Netherlands. Example library(dslabs) gapminder %&gt;% filter(country == &quot;Netherlands&quot;) %&gt;% ## select data from the Netherlands ggplot(aes(x = year, y = life_expectancy)) + geom_line() We add points to the graph by adding an extra geom_point() layer. Example gapminder %&gt;% filter(country == &quot;Netherlands&quot;) %&gt;% ## select data from the Netherlands ggplot(aes(x = year, y = life_expectancy)) + geom_line() + geom_point() Next, we compare the average life_expectancy per continent. Example gapminder %&gt;% group_by(year, continent) %&gt;% ## data is grouped for each continent mutate(life_exp = mean(life_expectancy, na.rm = TRUE)) %&gt;% ggplot(aes(x = year, y = life_exp)) + geom_line(aes(colour = continent)) # no need to factorise as continent is text, not a number An important feature to show in a graph is the standard deviation (stdev) of your measurements. We will illustrate this with a repeated measures example. Let’s say we measured \\(Ca^{2+}\\) levels in mg/L in the same pond for 5 days after a mining incident. However, as the \\(Ca^{2+}\\) levels also change with rain levels, etc, we measure 6 times a day and calculate a daily average. You can just copy paste this code to generate some fake data, we will discuss how to do these kind of steps in following lessons. # generate some fake data set.seed(12345) CA_norm_rand &lt;- c(round(rnorm(6,600,runif(1,20,50)),0), round(rnorm(6,580,runif(1,10,40)),0), round(rnorm(6,370,runif(1,10,30)),0), round(rnorm(6,300,runif(1,10,50)),0), round(rnorm(6,320,runif(1,10,30)),0) ) CA_data&lt;- tibble(day = rep(seq(1:5),each=6), measurement = rep(seq(1:6),5), CA=CA_norm_rand) CA_perday &lt;- CA_data %&gt;% group_by(day) %&gt;% summarise(mean_CA = mean(CA), stdev_CA = sd(CA)) Now we will make the graph. Make sure you do understand these lines: CA_perday %&gt;% ggplot(aes(x = day, y = mean_CA)) + geom_line() + geom_point() + geom_errorbar(aes(ymin = mean_CA - stdev_CA, ymax = mean_CA + stdev_CA), width = 0.2, colour = &quot;black&quot;)+ labs( title = &quot;CA2+ levels in the duck pond&quot;, subtitle= &quot;20-25 november 2021&quot;, x= &quot;days after incident&quot;, y= &quot;average Ca2+ concentration (mg/L)&quot;)+ theme_classic(base_size = 12) + # text size and overall theme theme(axis.text.y = element_text(size=12), #font y axis tick labels axis.text.x = element_text(size=12), #font x axis tick labels axis.title.x = element_text(vjust = -1)) #lower x-axis title a little –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; Exercise 4 Below are the measurements of an experiment on hand trembling as a side effect of using a new drug (TBA) at different doses (in nM). Trembling frequency (in Hz) was measured in triplo. conc_nM meting1 meting2 meting3 0 1.1 1.2 0.8 25 3.1 2.9 3.4 50 5.1 5.3 5.0 100 6.1 5.7 6.1 150 7.1 7.2 6.8 200 8.1 8.1 7.7 Put the table into R as a tibble. Make the data tidy. Click for the answer trembling &lt;- tibble(conc_nM = c(0, 25, 50, 100, 150, 200), meting1 = c(1.1,3.1,5.1,6.1,7.1,8.1), meting2 = c(1.2,2.9,5.3,5.7,7.2,8.1), meting3 = c(0.8,3.4,5.0,6.1,6.8,7.7)) trembling_tidy &lt;- trembling %&gt;% pivot_longer(cols = c(&quot;meting1&quot;, &quot;meting2&quot;, &quot;meting3&quot;), names_to = &quot;measurement&quot;, values_to = &quot;trembling_Hz&quot;) Exercise 4 Calculate the average and standard deviation of the trembling frequency for each concentration. Use the trembling_tidy object of the previous exercise in combination with the R functions group_by() and summarize() from the tidyverse package. We used them in lesson 2 to get the average “mpg” per amount of gears in the car dataset: example: as_tibble(mtcars) %&gt;% group_by(gear) %&gt;% summarize(average = mean(mpg, na.rm = TRUE)) ## # A tibble: 3 × 2 ## gear average ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3 16.1 ## 2 4 24.5 ## 3 5 21.4 Use group_by() on the conc_nM variable of object trembling_tidy Make a line graph of the average values and standard deviation. Click for the answer trem_line_mean_sd &lt;- trembling_tidy %&gt;% group_by(conc_nM) %&gt;% summarize(mean = mean(trembling_Hz, na.rm = TRUE), stdev = sd(trembling_Hz, na.rm = TRUE)) ggplot(data = trem_line_mean_sd, aes(x = conc_nM, y = mean)) + geom_point() + geom_line() + geom_errorbar(aes(ymin = mean - stdev, ymax = mean + stdev), width = 5, colour = &quot;black&quot;)+ labs( title = &quot;Hand trembling at different doses of medicine TBA&quot;, subtitle = &quot;errorbars denote 1 standard deviation&quot;, x= &quot;average TBA concentration (nM)&quot;, y= &quot;Hand trembling (Hz)&quot; )+ theme_classic(base_size = 12) + # text size and overall theme theme(axis.text.y = element_text(size=12), #font y axis tick labels axis.text.x = element_text(size=12), #font x axis tick labels axis.title.x = element_text(vjust = -1)) #lower x-axis title a little Boxplots A boxplot (box and whisker plot) is a graph that shows the spread of quantitative data based on quartiles and outliers (see figure below). Quartiles are just the datapoints, devided in 4 sections of equal amounts of datapoint. So the upper value of the lowest 25% of datapoints is the 25th percentile. The upper value of the lowest 50% of datapoints is the median. The upper value of the lowest 75% of datapoints is the 75th percentile. Figure 8: Interpretation of a boxplot. IQR = Inter Quartile Range. Now R calculates the difference between the 75th percentile and the 25th percentile. This is the Interquartile range (IQR). The upper whisker (vertical stripe) extends from the hinge (the box) to the largest value no further than 1.5 * IQR from the hinge. So 1.5 * IQR above the 75th percentile on top, and 1.5 * IQR below the 25th percentile on the bottom. To create a boxplot, we can use the function geom_boxplot(). We will use the airquality dataset which is built-in in R and contains daily air quality measurements in New York, May to September 1973. The variable “Ozone” contains mean ozone concentration in parts per billion from 13:00 to 15:00 hours at Roosevelt Island. We will put the data in a tibble: airdf &lt;- as.tibble(airquality) head(airdf) ## # A tibble: 6 × 6 ## Ozone Solar.R Wind Temp Month Day ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 41 190 7.4 67 5 1 ## 2 36 118 8 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 We can see that “Month” is saved as a number. However, suppose we want to group measurements per month and create a box for each month. We have to tell R to treat Month as factor rather than numeric with as.factor(). Example ggplot(data = airdf, aes(x = as.factor(Month), y = Ozone)) + geom_boxplot(aes(fill = as.factor(Month))) Exercise 4 ggplot has been giving us an warning: Warning message: Removed 37 rows containing non-finite values (stat_boxplot). Why? Click for the answer There are 37 missing values in the “Ozone” column. sum(is.na(airdf$Ozone)) ## [1] 37 Box plots can be used to look for outliers. Outliers are datapoints that differ significantly from the distribution of the rest of the data. In boxplots, any datapoint that is more than 1.5 * IQR above the 75th percentile or 1.5 * IQR below the 25th percentile is considered a potential outlier. They are plotted as individual points. To change to colour of the potential outliers use outlier.colour. To change to shape of the outliers use outlier.shape. Example ggplot(data = airdf, aes(x = as.factor(Month), y = Ozone)) + geom_boxplot(aes(fill = as.factor(Month)), outlier.colour = &quot;red&quot;, outlier.shape = 8) ## Warning: Removed 37 rows containing non-finite values (stat_boxplot). Boxplots are often presented with flipped coordinates. Also, a legend here is not needed: Example ggplot(data = airdf, aes(x = as.factor(Month), y = Ozone)) + geom_boxplot(aes(fill = as.factor(Month)), outlier.colour = &quot;red&quot;, outlier.shape = 8,show.legend = FALSE)+ coord_flip() ## Warning: Removed 37 rows containing non-finite values (stat_boxplot). Now this isn’t a usable graph. To finish your graph for communication, informative labels are required as well as putting some effort into making it look nice! Example ggplot(data = airdf, aes(x = as.factor(Month), y = Ozone)) + geom_boxplot(aes(fill = as.factor(Month)), outlier.colour = &quot;red&quot;, outlier.shape = 8,show.legend = FALSE) + scale_x_discrete( labels=c(&quot;May&quot;,&quot;June&quot;,&quot;July&quot;,&quot;August&quot;,&quot;September&quot;)) + theme_classic(base_size = 12) + # text size and overall theme theme(axis.text.y = element_text(size=12, angle = 0, vjust = 0), #font y axis tick labels axis.text.x = element_text(size=12, angle = 0, vjust = 0), #font x axis tick labels axis.title.x = element_text(vjust = -2))+ #lower x-axis title a little labs( title = &quot;Mean ozone concentration at Roosevelt Island&quot;, subtitle= &quot;per day, measured 1300 to 1500h, May to September 1973&quot;, x= &quot;Month&quot;, y= &quot;average [Ozone] (ppb)&quot; ) ## Warning: Removed 37 rows containing non-finite values (stat_boxplot). Exercise 4 Use the built-in dataset ‘iris’ to answer the following question. Make a boxplot showing the summary data of the variable “Sepal.Lenght” for the different iris species listed in the “Species” variable. Each boxplot has a different color. Click for the answer ggplot(data = iris, aes(x = Species, y = Sepal.Length)) + geom_boxplot(aes(fill = Species))+ labs( title = &quot;Sepal Lenght of different iris species&quot;, y= &quot;Sepal Length&quot; )+ theme(legend.position = &quot;none&quot;, text = element_text(size=16)) Outliers Potential outliers are easy to spot in a boxplot. Outliers can be an underlying reason that a Shapiro-Wilk test is telling you the data are not normally distributed, while you would have expected normally distributed data. When doing data analyis, you will have to decide on what to do with potential outliers. A common strategy is to set boundaries within which you consider data points valid data, and reject datapooints that are outside of your boundaries. Similar to the approach in the box plots. But there are other possibilities: you can for instance keep all your data but use non-parametric statistical tests. And there are a couple of nullhypothesis-tests that investigate whether datapoints are outliers or not. In this example we will follow the boxplot, and consider any point further than 1.5*IQR from the 25th or 75th percentile an outlier. Let’s say we measured the ears of 22 white rabbits: white_rabbit_ears= tibble(ear_length=c(17.5, 13.0, 18.6, 19.4, 20.9, 18.9, 22.8, 20.3, 62.1, 20.4, 23.0, 24.1, 26.1, 25.2, 27.8, 28.5, 76.5, 21.5, 20.2, 18.8)) ggplot(data = white_rabbit_ears, aes( y = ear_length)) + geom_boxplot()+ labs( title = &quot;Ear Lenght of white rabbits&quot;, y= &quot;ear Length (cm)&quot; )+ theme(legend.position = &quot;none&quot;, text = element_text(size=16)) Now if we want to remove those 2 outliers: Q1 &lt;- quantile(white_rabbit_ears$ear_length, .25) Q3 &lt;- quantile(white_rabbit_ears$ear_length, .75) IQR&lt;- IQR(white_rabbit_ears$ear_length) # get threshold values for outliers Tlower = Q1-(1.5*IQR) Tupper = Q3+(1.5*IQR) # replace outliers with NA white_rabbit_ears$ear_length[white_rabbit_ears$ear_length &lt; Tlower] &lt;- NA white_rabbit_ears$ear_length[white_rabbit_ears$ear_length &gt; Tupper] &lt;- NA Important : always report very precisely how many datapoints you excluded and using what method and thresholds! Also, be conservative in throwing away data. Are you really sure it is an outlier? In this case we are pretty sure. Those would be some very weid rabbits with ears around 70 cm. This is probably a measurement error or some weird mutant mega rabbits that shouldn’t be part of our sample. We can safely ignore these two data points. Relation Questions Instead of hunting for differences, sometimes we have questions regarding the relation between two variables. Usually, these cases are either correlational studies or regressions. They are similar, but not the same. In a correlation analysis, you investigate the relationship between two variables. A pearson’s correlation coefficient (r) is a number describing the linear relationship between two random variables X and Y. In a regression analysis, you try to find a model (equation) that describes a relationship between two variables X and Y (with Y being random but not X) using the least squares method. For instance in linear regression, this model would look like Y=aX+b. A model can then be used to predict Y if you have a value for X. Correlation We will investigate correlation with the penguins dataset. We want to explore the correlation between the flipper length and body mass in penguins. Do larger penguins have larger flippers? 1. load data, prepare data library(palmerpenguins) head(penguins) ## # A tibble: 6 × 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 male 2007 ## 2 Adelie Torgersen 39.5 17.4 186 3800 female 2007 ## 3 Adelie Torgersen 40.3 18 195 3250 female 2007 ## 4 Adelie Torgersen NA NA NA NA &lt;NA&gt; 2007 ## 5 Adelie Torgersen 36.7 19.3 193 3450 female 2007 ## 6 Adelie Torgersen 39.3 20.6 190 3650 male 2007 2. plot using scatterplot This kind of data should be plotted with a scatter plot. A scatter plot is a graph in which the values of two continous variables are plotted along x-axis and y-axis. The pattern of the resulting points could reveal any correlation present. To create a scatter plot we use the function geom_point(). ggplot(data = penguins, aes(x = body_mass_g, y = flipper_length_mm)) + geom_point()+ labs(title = &quot;Relation between flipper length and body mass&quot;, subtitle = &quot; Penguins, Palmer Station LTER&quot;, y = &quot;Flipper length (mm)&quot;, x = &quot;Body mass (g)&quot;) + theme_minimal() ## Warning: Removed 2 rows containing missing values (geom_point). At first glance, it seems that heavier penguins have longer flippers. The graph shows a increasing relation between flipper length in mm and body mass in gram, as you see the dots generally being higher on the y-axis with lower body weights (more to the right on the x-axis). In other words how heavier the penguin, the larger its flippers. 3. perform correlation analysis We can check whether there is indeed a correlation with cor.test(). cor.test() can do several types of correlation analyses, including the most common: Pearson correlation. Pearson’s correlation is a measure of the linear relationship between two continuous random variables. Pearson correlation does not strictly require the data itself to be normally distributed, in reality it is a bit more complicated, especially with larger datasets. The ins and outs go beyond the scope of this course. A pearson correlation analysis can be done like this: cor.test(penguins$body_mass_g, penguins$flipper_length_mm, method=c(&quot;pearson&quot;)) ## ## Pearson&#39;s product-moment correlation ## ## data: penguins$body_mass_g and penguins$flipper_length_mm ## t = 32.722, df = 340, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.843041 0.894599 ## sample estimates: ## cor ## 0.8712018 It seems that there is indeed a significant correlation between flipper length and body mass because p is smaller than 0.05. The correlation coefficient is 0.8712018. However, there are several penguin species in this dataset. Let’s colour the points in the scatterplot based on the species: ggplot(data = penguins, aes(x = body_mass_g, y = flipper_length_mm)) + geom_point(aes(color = species, shape = species), size = 3,alpha = 0.8)+ labs(title = &quot;Relation between flipper length and body mass&quot;, subtitle = &quot; Penguins, Palmer Station LTER&quot;, y = &quot;Flipper length (mm)&quot;, x = &quot;Body mass (g)&quot;) + theme_minimal() ## Warning: Removed 2 rows containing missing values (geom_point). We can further modify the graph by changing the shape, size and alpha (= transparency) of the points. Let’s also put the correlation coefficient in the graph with annotate(). # round correlation coefficient to 1 decimal place cor_coefficient &lt;- round(cor.test(penguins$body_mass_g, penguins$flipper_length_mm, method=c(&quot;pearson&quot;))$estimate,1) # add to plot ggplot(data = penguins, aes(x = body_mass_g, y = flipper_length_mm)) + geom_point(aes(color = species, shape = species), size = 3,alpha = 0.8)+ labs(title = &quot;Relation between flipper length and body mass&quot;, subtitle = &quot; Penguins, Palmer Station LTER&quot;, y = &quot;Flipper length (mm)&quot;, x = &quot;Body mass (g)&quot;) + theme_minimal()+ annotate(&quot;text&quot;, x = 3500, y = 230, size=4, label = paste(&quot;pearson&#39;s r = &quot;, cor_coefficient)) ## Warning: Removed 2 rows containing missing values (geom_point). 4. draw a conclusion Because p &lt; 0.05, the analysis show that there is a significant correlation between flipper length and body mass (p&lt;0.001)*, with a correlation coefficient of 0.9. As can be seen in the graph, penguins with a larger body mass tend to have longer flippers. with very low values for p, you report “p&lt;0.01” or “p&lt;0.001” instead of the exact p-value. Anything above 0.001 is just reported exactly. Don’t confuse the reporting of the p-value (e.g. “p=0.032” or “p&lt;0.001”) with the testing of the p-value against an \\(/alpha\\) of 0.05. Exercise 4 Use dataset ‘temp_carbon’ from the dslabs package to answer the following questions. What are the units used for the variables carbon_emissions and temp_anomaly? use ?temp_carbon once you loaded the package. Make a scatter plot of carbon_emissions (x-axis) and temp_anomaly (y-axis). tip: if an axis title gets too long, you can add a line break by using \\n. eg: labs(y = \"text text \\nmoretext\") Rewrite the code of (a) to show ocean_anomaly and land anomaly as separate colours in the graph. Click for the answer carbon_emissions: millions of metric tons temp_anomaly: degrees Celsius library(dslabs) ggplot(data = temp_carbon, aes(x = carbon_emissions, y = temp_anomaly)) + geom_point()+ labs(title = &quot;Annual mean global temperature anomaly versus carbon emissions. &quot;, x = &quot;Annual carbon emissions (* million metric tons)&quot;, y = &quot; Global annual mean temperature anomaly in degrees Celsius \\nrelative to the 20th century mean&quot;) ## Warning: Removed 133 rows containing missing values (geom_point). (c) ggplot(data = temp_carbon, aes(x = carbon_emissions)) + geom_point(aes(y = ocean_anomaly), colour = &quot;blue&quot;) + geom_point(aes(y = land_anomaly), colour = &quot;red&quot;) + labs(title = &quot;Annual mean temperature anomaly over oceans and on land \\nversus carbon emissions. &quot;, x = &quot;Annual carbon emissions (* million metric tons)&quot;, y = &quot;annual mean temperature anomaly in degrees Celsius \\nrelative to the 20th century mean&quot;) ## Warning: Removed 133 rows containing missing values (geom_point). ## Removed 133 rows containing missing values (geom_point). Exercise 4 Use dataset ‘temp_carbon’ from the dslabs package to answer the following questions. Make the dataset temp_carbon tidy. Use the tidy dataset to re-create the figure of the previous exercise, part (c) (the colours don’t need to be exaclty the same) Click for the answer temp_carb_tidy &lt;- temp_carbon %&gt;% pivot_longer(cols = c(&quot;land_anomaly&quot;, &quot;ocean_anomaly&quot;), names_to = &quot;anomaly&quot;, values_to = &quot;temperature&quot;) temp_carb_tidy %&gt;% ggplot(aes(x = carbon_emissions, y = temperature, colour = anomaly)) + geom_point() + labs(title = &quot;Annual mean temperature anomaly over oceans and on land \\nversus carbon emissions. &quot;, x = &quot;Annual carbon emissions (* million metric tons)&quot;, y = &quot;annual mean temperature anomaly in degrees Celsius \\nrelative to the 20th century mean&quot;) ## Warning: Removed 266 rows containing missing values (geom_point). Clearly, working with tidy data makes things easier. We don’t need to add the ocean- and land-points separately. Regression Scatter plots are often used in the lab to make a calibration curve (“ijklijn”). We can use R again to make a calibration curve, again with the ggpubr package that you installed earlier. A calibration curve is an example of a regression analysis. We try to find an equation (Y=aX+b) that describes a relationship between two variables X and Y. We make a scatterplot with ggplot(), and add both the determination coefficient (= the squared correlation coefficient (\\(R^2\\)) ) with stat_cor() and the equation (Y = b + a*X, with a=slope and b=intercept) with stat_regline_equation(). 1. load data, prepare data # load packages library(ggplot2) library(ggpubr) # generate some fake data set.seed(123) df &lt;- data.frame(x = c(1:10)) # X is not a random variable df$y &lt;- 2 + 3 * df$x + rnorm(10, sd = 0.8) 2. plot using scatterplot, include trendline &amp; 3. perform regression analysis # make calibration line ggplot(data = df, aes(x = x, y = y)) + geom_smooth(method = &quot;lm&quot;, se=FALSE, color=&quot;blue&quot;, formula = y ~ x) + geom_point()+ stat_cor(label.y = 60, size = 5,digits = 3, aes(label = paste(..rr.label.., sep = &quot;~`,`~&quot;))) + stat_regline_equation(label.y = 55, size = 5)+ theme_classic(base_size = 18) + # text size and overall theme labs(title = &quot;Calibration line for X&quot;, x = &quot;X (units)&quot;, y = &quot;Y (units)&quot;) 4. use model for whatever you like Now you could use the model to predict the Y value for a data point with X=5.4. What would it be? Click for the answer round(1.5*(3.1*5.4),1) ## [1] 25.1 And the X value for a data point with Y=15? Click for the answer round((15-1.5)/3.1,1) ## [1] 4.4 Checklists for statistical tests and visualisations verschilvragen one sample design load data, prepare data plot using histogram or boxplot check normality (Shapio-Wilk) perform one sample t-test draw a conclusion two sample design Look at the experimental design to see if it is paired or unpaired paired: load data, prepare data plot using bargraph or boxplot check normality (Shapio-Wilk) of difference between conditions perform two sample t-test draw a conclusion unpaired: load data, prepare data plot using bargraph or boxplot check normality (Shapio-Wilk) of both groups separately check equality of variances (Shapio-Wilk) perform two sample t-test draw a conclusion ANOVA (&gt; 2 conditions) load data, prepare data plot using bargraph or boxplot check normality (Shapio-Wilk) of both groups separately perform ANOVA if significant, perform post hoc tests verbandvragen correlation load data, prepare data plot using scatterplot perform correlation analysis draw a conclusion regression load data, prepare data plot using scatterplot, include trendline perform regression analysis use model for whatever you like Overview R functions To create a graph function package ggplot() tidyverse geom_bar() tidyverse geom_col() tidyverse geom_point() tidyverse geom_smooth tidyverse geom_line() tidyverse geom_errorbar tidyverse coord_flip() tidyverse labs() tidyverse Data transformation function package mutate() tidyverse reorder() stats filter () tidyverse group_by() tidyverse Arithmetic function package count() tidyverse mean() base "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
